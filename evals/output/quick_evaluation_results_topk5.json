{
  "summary": {
    "total_questions": 10,
    "average_f1_score": 0.3294593607060359,
    "average_llm_as_judge_score": 0.915,
    "average_latency": 10.20872163772583,
    "average_mean_cosine_score": 0.6580810015059387,
    "question_type_breakdown": {
      "factual": {
        "count": 7,
        "avg_f1_score": 0.3244399192473635,
        "avg_llm_as_judge_score": 0.9142857142857144,
        "avg_latency": 11.558936357498169,
        "avg_precision_at_k": 0.20840169583693954,
        "avg_recall_at_k": 0.7384986895190977,
        "avg_mean_cosine": 0.6308600228653779
      },
      "analytical": {
        "count": 2,
        "avg_f1_score": 0.42407936104592353,
        "avg_llm_as_judge_score": 1.0,
        "avg_latency": 8.913509845733643,
        "avg_precision_at_k": 0.26917860438987196,
        "avg_recall_at_k": 1.0,
        "avg_mean_cosine": 0.7498280064907018
      },
      "conceptual": {
        "count": 1,
        "avg_f1_score": 0.17535545023696683,
        "avg_llm_as_judge_score": 0.75,
        "avg_latency": 3.347642183303833,
        "avg_precision_at_k": 0.11526479750778816,
        "avg_recall_at_k": 0.36633663366336633,
        "avg_mean_cosine": 0.6651338420203396
      }
    },
    "overall_score": 0.6435596037768314
  },
  "detailed_results": [
    {
      "question": "What are the main improvements introduced in the Seedance 1.0 video generation model?",
      "expected_answer": "Seedance 1.0 introduces several core technical improvements, including multi-source data curation with meaningful video captioning for comprehensive learning across diverse scenarios, and an efficient architecture design with a proposed training paradigm that supports multi-shot generation and jointly learns text-to-video and image-to-video tasks.",
      "actual_answer": "The Seedance 1.0 video generation model introduces several key improvements:\n\n1. **Multi-Source Data and Comprehensive Video Captioning**: The model uses a large-scale, high-quality video dataset curated from multiple sources, with comprehensive video captioning that enables learning across diverse scenarios and accurate interpretation of user instructions.\n\n2. **Efficient Architecture Design**: It features a design that decouples spatial and temporal layers with interleaved multimodal positional encoding, supporting multi-shot generation and joint learning of both text-to-video and image-to-video tasks.\n\n3. **Post-Training Optimization**: The model leverages carefully-optimized post-training approaches, including fine-grained supervised fine-tuning and video-specific reinforcement learning with multi-dimensional reward mechanisms, enhancing performance.\n\n4. **Inference Acceleration**: A multi-stage distillation framework and system-level optimizations reduce the number of function evaluations required for generation, achieving over a 10x speedup in inference time without degrading model performance.\n\nThese improvements enable Seedance 1.0 to achieve superior spatiotemporal coherence, structural stability, motion fluidity, and photorealistic visual quality, while precisely following complex user instructions.",
      "expected_correct_chunks": "arXiv:2506.09113v2 [cs.CV] 28 Jun 2025 Seedance 1.0: Exploring the Boundaries of Video Generation Models ByteDance Seed Abstract Notable breakthroughs in diffusion modeling have propelled rapid improvements in video generation, yet current foundational model still face critical challenges in simultaneously balancing prompt following, motion plausibility, and visual quality. In this report, we introduce Seedance 1.0 , a high-performance and inference-efficient video foundation generation model that integrates several core technical improvements: (i) multi-source data curation augmented with precision and meaningful video captioning, enabling comprehensive learning across diverse scenarios; (ii) an efficient architecture design with proposed training paradigm, which allows for natively supporting multi-shot generation and jointly learning of both text-to-video and image-to-video tasks",
      "sources": [
        "Seedance 1.0 - 2506.09113v2",
        "Seedance 1.0 - 2506.09113v2",
        "Seedance 1.0 - 2506.09113v2",
        "Seedance 1.0 - 2506.09113v2",
        "Seedance 1.0 - 2506.09113v2"
      ],
      "chunk_ids": [
        "86b80d8365a047c18c4571b255bb6c48",
        "4368ad1653d64bb8b8f2830de1f93734",
        "d3b2c3e23e8e4d91b15587460ea852f8",
        "5dee9baa42144209b6fda8976cbe070b",
        "28e44c0d24424a10bc5c83cecee9b921"
      ],
      "f1_score": 0.44028103044496486,
      "precision_at_k": 0.2822822822822823,
      "recall_at_k": 1.0,
      "mean_cosine_score": 0.7615411564591787,
      "llm_as_judge_score": 1.0,
      "latency": 11.589494943618774,
      "retrieved_chunks_text": ". (iii) carefully-optimized post-training approaches leveraging fine-grained supervised fine-tuning, and video-specific RLHF with multi-dimensional reward mechanisms for comprehensive performance improvements; (iv) excellent model acceleration achieving 10 ×inference speedup through multi- stage distillation strategies and system-level optimizations. Seedance 1.0 can generate a 5-second video at 1080p resolution only with 41.4 seconds (NVIDIA-L20). Compared to state-of-the-art video generation models, Seedance 1.0 stands out with high-quality and fast video generation having superior spatiotemporal fluidity with structural stability, precise instruction adherence in complex multi-subject contexts, native multi-shot narrative coherence with consistent subject representation. Seedance 1.0 is now accessible on Volcano Engineα. Official Page: https://seed.bytedance.com/seedance αModel ID: Doubao-Seedance-1.0-pro Figure 1 Overall evaluation. Left: Text-to-Video; Right: Image-to-Video\n. 21 7.5 Visualization We present several visual outcomes by Seedance 1.0 in Figure 14,15,16. For additional examples, please refer to the official website for an enhanced viewing experience. 8 Conclusion We have introduced Seedance 1.0, a native bilingual video generation foundation model that unifies multiple generation paradigms (such as text-to-video and image-to-video) and excels in instruction following, motion stability, and visual quality. We presented our technical improvements in dataset construction, efficient architecture design with training paradigm, post-training optimization, and inference acceleration, which are integrated effectively to achieve a high-performance model with fast inference. It demonstrates excellent capabilities in handling complex scenarios, multi-shot generation, and long-range temporal coherence, all while delivering fast and photorealistic generation experiences. 22 References [1] artificialanalysis.ai. artificialanalysis\narXiv:2506.09113v2 [cs.CV] 28 Jun 2025 Seedance 1.0: Exploring the Boundaries of Video Generation Models ByteDance Seed Abstract Notable breakthroughs in diffusion modeling have propelled rapid improvements in video generation, yet current foundational model still face critical challenges in simultaneously balancing prompt following, motion plausibility, and visual quality. In this report, we introduce Seedance 1.0 , a high-performance and inference-efficient video foundation generation model that integrates several core technical improvements: (i) multi-source data curation augmented with precision and meaningful video captioning, enabling comprehensive learning across diverse scenarios; (ii) an efficient architecture design with proposed training paradigm, which allows for natively supporting multi-shot generation and jointly learning of both text-to-video and image-to-video tasks\n. •Inference Acceleration. We proposed a multi-stage distillation framework to reduce the number of function evaluations (NFE) required for generation, with inference infrastructure optimization techniques, achieving over 10 ×end-to-end speedup with no degradation in model performance. Compared with contemporary models, Seedance 1.0 exhibits four distinguishing characteristics: •Comprehensive Generation Capabilities. Seedance 1.0 achieves superior spatiotemporal coherence and structural stability, demonstrating exceptional motion fluidity and physical plausibility. The model produces photorealistic visuals with nuanced textures and compositional richness, attaining state-of-the-art performance across both proprietary evaluation suites and authoritative third-party benchmarks. •Precision Instruction Following. Through comprehensive learning of diverse scenarios, entities, and action semantics, Seedance 1.0 precisely interprets complex user specifications\n. To address these limitations, we present Seedance 1.0 , a foundational video generation model with native support bilingual (Chinese/English) video generation and multi-task versatility encompassing text-to-video synthesis and image-guided video generation. Seedance 1.0 integrates four key technical improvements: •Multi-Source Data with Comprehensive Video Captioning. Through multi-stage, multi-perspective curation and dataset balancing, we construct a large-scale high-quality video dataset spanning diverse categories, styles, and sources. This enables a comprehensive learning of rich scenarios, topics, and action dynamics. Our precision video captioning system ensures accurate interpretation of user instructions while enabling fluent generation of complex video narratives. •Efficient Architecture Design. In our design, we decouple spatial and temporal layers with an interleaved multimodal positional encoding",
      "retrieved_chunks": [
        {
          "chunk_id": "86b80d8365a047c18c4571b255bb6c48",
          "source": "Seedance 1.0 - 2506.09113v2",
          "chunk_text": ". (iii) carefully-optimized post-training approaches leveraging fine-grained supervised fine-tuning, and video-specific RLHF with multi-dimensional reward mechanisms for comprehensive performance improvements; (iv) excellent model acceleration achieving 10 ×inference speedup through multi- stage distillation strategies and system-level optimizations. Seedance 1.0 can generate a 5-second video at 1080p resolution only with 41.4 seconds (NVIDIA-L20). Compared to state-of-the-art video generation models, Seedance 1.0 stands out with high-quality and fast video generation having superior spatiotemporal fluidity with structural stability, precise instruction adherence in complex multi-subject contexts, native multi-shot narrative coherence with consistent subject representation. Seedance 1.0 is now accessible on Volcano Engineα. Official Page: https://seed.bytedance.com/seedance αModel ID: Doubao-Seedance-1.0-pro Figure 1 Overall evaluation. Left: Text-to-Video; Right: Image-to-Video"
        },
        {
          "chunk_id": "4368ad1653d64bb8b8f2830de1f93734",
          "source": "Seedance 1.0 - 2506.09113v2",
          "chunk_text": ". 21 7.5 Visualization We present several visual outcomes by Seedance 1.0 in Figure 14,15,16. For additional examples, please refer to the official website for an enhanced viewing experience. 8 Conclusion We have introduced Seedance 1.0, a native bilingual video generation foundation model that unifies multiple generation paradigms (such as text-to-video and image-to-video) and excels in instruction following, motion stability, and visual quality. We presented our technical improvements in dataset construction, efficient architecture design with training paradigm, post-training optimization, and inference acceleration, which are integrated effectively to achieve a high-performance model with fast inference. It demonstrates excellent capabilities in handling complex scenarios, multi-shot generation, and long-range temporal coherence, all while delivering fast and photorealistic generation experiences. 22 References [1] artificialanalysis.ai. artificialanalysis"
        },
        {
          "chunk_id": "d3b2c3e23e8e4d91b15587460ea852f8",
          "source": "Seedance 1.0 - 2506.09113v2",
          "chunk_text": "arXiv:2506.09113v2 [cs.CV] 28 Jun 2025 Seedance 1.0: Exploring the Boundaries of Video Generation Models ByteDance Seed Abstract Notable breakthroughs in diffusion modeling have propelled rapid improvements in video generation, yet current foundational model still face critical challenges in simultaneously balancing prompt following, motion plausibility, and visual quality. In this report, we introduce Seedance 1.0 , a high-performance and inference-efficient video foundation generation model that integrates several core technical improvements: (i) multi-source data curation augmented with precision and meaningful video captioning, enabling comprehensive learning across diverse scenarios; (ii) an efficient architecture design with proposed training paradigm, which allows for natively supporting multi-shot generation and jointly learning of both text-to-video and image-to-video tasks"
        },
        {
          "chunk_id": "5dee9baa42144209b6fda8976cbe070b",
          "source": "Seedance 1.0 - 2506.09113v2",
          "chunk_text": ". •Inference Acceleration. We proposed a multi-stage distillation framework to reduce the number of function evaluations (NFE) required for generation, with inference infrastructure optimization techniques, achieving over 10 ×end-to-end speedup with no degradation in model performance. Compared with contemporary models, Seedance 1.0 exhibits four distinguishing characteristics: •Comprehensive Generation Capabilities. Seedance 1.0 achieves superior spatiotemporal coherence and structural stability, demonstrating exceptional motion fluidity and physical plausibility. The model produces photorealistic visuals with nuanced textures and compositional richness, attaining state-of-the-art performance across both proprietary evaluation suites and authoritative third-party benchmarks. •Precision Instruction Following. Through comprehensive learning of diverse scenarios, entities, and action semantics, Seedance 1.0 precisely interprets complex user specifications"
        },
        {
          "chunk_id": "28e44c0d24424a10bc5c83cecee9b921",
          "source": "Seedance 1.0 - 2506.09113v2",
          "chunk_text": ". To address these limitations, we present Seedance 1.0 , a foundational video generation model with native support bilingual (Chinese/English) video generation and multi-task versatility encompassing text-to-video synthesis and image-guided video generation. Seedance 1.0 integrates four key technical improvements: •Multi-Source Data with Comprehensive Video Captioning. Through multi-stage, multi-perspective curation and dataset balancing, we construct a large-scale high-quality video dataset spanning diverse categories, styles, and sources. This enables a comprehensive learning of rich scenarios, topics, and action dynamics. Our precision video captioning system ensures accurate interpretation of user instructions while enabling fluent generation of complex video narratives. •Efficient Architecture Design. In our design, we decouple spatial and temporal layers with an interleaved multimodal positional encoding"
        }
      ],
      "metadata": {
        "question_type": "factual",
        "generated_by": "gpt-4o-mini",
        "top_k": 5,
        "retrieved_chunks_array": [
          {
            "chunk_id": "86b80d8365a047c18c4571b255bb6c48",
            "source": "Seedance 1.0 - 2506.09113v2",
            "chunk_text": ". (iii) carefully-optimized post-training approaches leveraging fine-grained supervised fine-tuning, and video-specific RLHF with multi-dimensional reward mechanisms for comprehensive performance improvements; (iv) excellent model acceleration achieving 10 ×inference speedup through multi- stage distillation strategies and system-level optimizations. Seedance 1.0 can generate a 5-second video at 1080p resolution only with 41.4 seconds (NVIDIA-L20). Compared to state-of-the-art video generation models, Seedance 1.0 stands out with high-quality and fast video generation having superior spatiotemporal fluidity with structural stability, precise instruction adherence in complex multi-subject contexts, native multi-shot narrative coherence with consistent subject representation. Seedance 1.0 is now accessible on Volcano Engineα. Official Page: https://seed.bytedance.com/seedance αModel ID: Doubao-Seedance-1.0-pro Figure 1 Overall evaluation. Left: Text-to-Video; Right: Image-to-Video"
          },
          {
            "chunk_id": "4368ad1653d64bb8b8f2830de1f93734",
            "source": "Seedance 1.0 - 2506.09113v2",
            "chunk_text": ". 21 7.5 Visualization We present several visual outcomes by Seedance 1.0 in Figure 14,15,16. For additional examples, please refer to the official website for an enhanced viewing experience. 8 Conclusion We have introduced Seedance 1.0, a native bilingual video generation foundation model that unifies multiple generation paradigms (such as text-to-video and image-to-video) and excels in instruction following, motion stability, and visual quality. We presented our technical improvements in dataset construction, efficient architecture design with training paradigm, post-training optimization, and inference acceleration, which are integrated effectively to achieve a high-performance model with fast inference. It demonstrates excellent capabilities in handling complex scenarios, multi-shot generation, and long-range temporal coherence, all while delivering fast and photorealistic generation experiences. 22 References [1] artificialanalysis.ai. artificialanalysis"
          },
          {
            "chunk_id": "d3b2c3e23e8e4d91b15587460ea852f8",
            "source": "Seedance 1.0 - 2506.09113v2",
            "chunk_text": "arXiv:2506.09113v2 [cs.CV] 28 Jun 2025 Seedance 1.0: Exploring the Boundaries of Video Generation Models ByteDance Seed Abstract Notable breakthroughs in diffusion modeling have propelled rapid improvements in video generation, yet current foundational model still face critical challenges in simultaneously balancing prompt following, motion plausibility, and visual quality. In this report, we introduce Seedance 1.0 , a high-performance and inference-efficient video foundation generation model that integrates several core technical improvements: (i) multi-source data curation augmented with precision and meaningful video captioning, enabling comprehensive learning across diverse scenarios; (ii) an efficient architecture design with proposed training paradigm, which allows for natively supporting multi-shot generation and jointly learning of both text-to-video and image-to-video tasks"
          },
          {
            "chunk_id": "5dee9baa42144209b6fda8976cbe070b",
            "source": "Seedance 1.0 - 2506.09113v2",
            "chunk_text": ". •Inference Acceleration. We proposed a multi-stage distillation framework to reduce the number of function evaluations (NFE) required for generation, with inference infrastructure optimization techniques, achieving over 10 ×end-to-end speedup with no degradation in model performance. Compared with contemporary models, Seedance 1.0 exhibits four distinguishing characteristics: •Comprehensive Generation Capabilities. Seedance 1.0 achieves superior spatiotemporal coherence and structural stability, demonstrating exceptional motion fluidity and physical plausibility. The model produces photorealistic visuals with nuanced textures and compositional richness, attaining state-of-the-art performance across both proprietary evaluation suites and authoritative third-party benchmarks. •Precision Instruction Following. Through comprehensive learning of diverse scenarios, entities, and action semantics, Seedance 1.0 precisely interprets complex user specifications"
          },
          {
            "chunk_id": "28e44c0d24424a10bc5c83cecee9b921",
            "source": "Seedance 1.0 - 2506.09113v2",
            "chunk_text": ". To address these limitations, we present Seedance 1.0 , a foundational video generation model with native support bilingual (Chinese/English) video generation and multi-task versatility encompassing text-to-video synthesis and image-guided video generation. Seedance 1.0 integrates four key technical improvements: •Multi-Source Data with Comprehensive Video Captioning. Through multi-stage, multi-perspective curation and dataset balancing, we construct a large-scale high-quality video dataset spanning diverse categories, styles, and sources. This enables a comprehensive learning of rich scenarios, topics, and action dynamics. Our precision video captioning system ensures accurate interpretation of user instructions while enabling fluent generation of complex video narratives. •Efficient Architecture Design. In our design, we decouple spatial and temporal layers with an interleaved multimodal positional encoding"
          }
        ]
      }
    },
    {
      "question": "What are the key features that differentiate Seedance 1.0 from other state-of-the-art video generation models?",
      "expected_answer": "Seedance 1.0 is distinguished by its high-quality and fast video generation, superior spatiotemporal fluidity with structural stability, precise instruction adherence in complex multi-subject contexts, and native multi-shot narrative coherence with consistent subject representation.",
      "actual_answer": "Seedance 1.0 stands out from other state-of-the-art video generation models due to several key features:\n\n1. **Multi-Source Data and Comprehensive Video Captioning**: Seedance 1.0 uses a large-scale, high-quality video dataset curated from diverse sources with precise video captioning. This enables comprehensive learning across various scenarios and ensures accurate interpretation of user instructions, allowing for the generation of complex video narratives.\n\n2. **Efficient Architecture Design**: The model features a design that decouples spatial and temporal layers with interleaved multimodal positional encoding. This architecture supports multi-shot generation and allows for the joint learning of both text-to-video and image-to-video tasks, making it versatile across different video generation paradigms.\n\n3. **Post-Training Optimization**: Seedance 1.0 uses fine-grained supervised fine-tuning and video-specific reinforcement learning with multi-dimensional reward mechanisms. These post-training optimizations enhance its performance significantly.\n\n4. **Inference Acceleration**: A multi-stage distillation framework and system-level optimizations achieve over a 10x speedup in inference time without degrading model performance. This allows for the quick generation of high-quality videos.\n\n5. **High-Quality and Fast Video Generation**: Seedance 1.0 produces videos with superior spatiotemporal fluidity, structural stability, and photorealistic visual quality. It excels in following complex user instructions and maintaining narrative coherence in multi-shot scenarios.\n\n6. **Bilingual Capability**: The model supports native bilingual video generation (Chinese/English), enhancing its usability across different language contexts.\n\nThese features collectively enable Seedance 1.0 to outperform other models, such as Veo 3, Kling 2.0, and others, in both text-to-video and image-to-video tasks, providing a unified approach to video generation that excels across multiple domains.",
      "expected_correct_chunks": ". (iii) carefully-optimized post-training approaches leveraging fine-grained supervised fine-tuning, and video-specific RLHF with multi-dimensional reward mechanisms for comprehensive performance improvements; (iv) excellent model acceleration achieving 10 ×inference speedup through multi- stage distillation strategies and system-level optimizations. Seedance 1.0 can generate a 5-second video at 1080p resolution only with 41.4 seconds (NVIDIA-L20). Compared to state-of-the-art video generation models, Seedance 1.0 stands out with high-quality and fast video generation having superior spatiotemporal fluidity with structural stability, precise instruction adherence in complex multi-subject contexts, native multi-shot narrative coherence with consistent subject representation. Seedance 1.0 is now accessible on Volcano Engineα. Official Page: https://seed.bytedance.com/seedance αModel ID: Doubao-Seedance-1.0-pro Figure 1 Overall evaluation. Left: Text-to-Video; Right: Image-to-Video",
      "sources": [
        "Seedance 1.0 - 2506.09113v2",
        "Seedance 1.0 - 2506.09113v2",
        "Seedance 1.0 - 2506.09113v2",
        "Seedance 1.0 - 2506.09113v2",
        "Seedance 1.0 - 2506.09113v2"
      ],
      "chunk_ids": [
        "86b80d8365a047c18c4571b255bb6c48",
        "4368ad1653d64bb8b8f2830de1f93734",
        "d3b2c3e23e8e4d91b15587460ea852f8",
        "28e44c0d24424a10bc5c83cecee9b921",
        "db78adc1f82e41e68a67b14e9ebc61ef"
      ],
      "f1_score": 0.4365256124721603,
      "precision_at_k": 0.2792022792022792,
      "recall_at_k": 1.0,
      "mean_cosine_score": 0.7767465117041477,
      "llm_as_judge_score": 1.0,
      "latency": 12.316600799560547,
      "retrieved_chunks_text": ". (iii) carefully-optimized post-training approaches leveraging fine-grained supervised fine-tuning, and video-specific RLHF with multi-dimensional reward mechanisms for comprehensive performance improvements; (iv) excellent model acceleration achieving 10 ×inference speedup through multi- stage distillation strategies and system-level optimizations. Seedance 1.0 can generate a 5-second video at 1080p resolution only with 41.4 seconds (NVIDIA-L20). Compared to state-of-the-art video generation models, Seedance 1.0 stands out with high-quality and fast video generation having superior spatiotemporal fluidity with structural stability, precise instruction adherence in complex multi-subject contexts, native multi-shot narrative coherence with consistent subject representation. Seedance 1.0 is now accessible on Volcano Engineα. Official Page: https://seed.bytedance.com/seedance αModel ID: Doubao-Seedance-1.0-pro Figure 1 Overall evaluation. Left: Text-to-Video; Right: Image-to-Video\n. 21 7.5 Visualization We present several visual outcomes by Seedance 1.0 in Figure 14,15,16. For additional examples, please refer to the official website for an enhanced viewing experience. 8 Conclusion We have introduced Seedance 1.0, a native bilingual video generation foundation model that unifies multiple generation paradigms (such as text-to-video and image-to-video) and excels in instruction following, motion stability, and visual quality. We presented our technical improvements in dataset construction, efficient architecture design with training paradigm, post-training optimization, and inference acceleration, which are integrated effectively to achieve a high-performance model with fast inference. It demonstrates excellent capabilities in handling complex scenarios, multi-shot generation, and long-range temporal coherence, all while delivering fast and photorealistic generation experiences. 22 References [1] artificialanalysis.ai. artificialanalysis\narXiv:2506.09113v2 [cs.CV] 28 Jun 2025 Seedance 1.0: Exploring the Boundaries of Video Generation Models ByteDance Seed Abstract Notable breakthroughs in diffusion modeling have propelled rapid improvements in video generation, yet current foundational model still face critical challenges in simultaneously balancing prompt following, motion plausibility, and visual quality. In this report, we introduce Seedance 1.0 , a high-performance and inference-efficient video foundation generation model that integrates several core technical improvements: (i) multi-source data curation augmented with precision and meaningful video captioning, enabling comprehensive learning across diverse scenarios; (ii) an efficient architecture design with proposed training paradigm, which allows for natively supporting multi-shot generation and jointly learning of both text-to-video and image-to-video tasks\n. To address these limitations, we present Seedance 1.0 , a foundational video generation model with native support bilingual (Chinese/English) video generation and multi-task versatility encompassing text-to-video synthesis and image-guided video generation. Seedance 1.0 integrates four key technical improvements: •Multi-Source Data with Comprehensive Video Captioning. Through multi-stage, multi-perspective curation and dataset balancing, we construct a large-scale high-quality video dataset spanning diverse categories, styles, and sources. This enables a comprehensive learning of rich scenarios, topics, and action dynamics. Our precision video captioning system ensures accurate interpretation of user instructions while enabling fluent generation of complex video narratives. •Efficient Architecture Design. In our design, we decouple spatial and temporal layers with an interleaved multimodal positional encoding\n. Seedance 1.0 has participated in both categories. Some notable external competitors include Veo 3, Kling 2.0, Runway Gen4, OpenAI Sora, and Wan 2.1. Seedance 1.0 tops both the text-to-video and image-to-video leaderboards, demonstrating a substantial performance advantage over competing models. In particular, it outperforms the second- and third-best models, Veo 3 and Kling 2.0, by over 100 points in the image-to-video task. Notably, Seedance 1.0 attains state-of-the-art results across both tasks using a single unified model, whereas prior models typically excelled in one domain while underperforming in the other. The subsequent sections provide a detailed analysis of Seedance 1.0’s advantages in each scenario. 7.2 Comprehensive Evaluation Besides overall user preferences, a comprehensive benchmark is equally important for the evaluation of visual generation models, as it enables a more holistic assessment of model capabilities",
      "retrieved_chunks": [
        {
          "chunk_id": "86b80d8365a047c18c4571b255bb6c48",
          "source": "Seedance 1.0 - 2506.09113v2",
          "chunk_text": ". (iii) carefully-optimized post-training approaches leveraging fine-grained supervised fine-tuning, and video-specific RLHF with multi-dimensional reward mechanisms for comprehensive performance improvements; (iv) excellent model acceleration achieving 10 ×inference speedup through multi- stage distillation strategies and system-level optimizations. Seedance 1.0 can generate a 5-second video at 1080p resolution only with 41.4 seconds (NVIDIA-L20). Compared to state-of-the-art video generation models, Seedance 1.0 stands out with high-quality and fast video generation having superior spatiotemporal fluidity with structural stability, precise instruction adherence in complex multi-subject contexts, native multi-shot narrative coherence with consistent subject representation. Seedance 1.0 is now accessible on Volcano Engineα. Official Page: https://seed.bytedance.com/seedance αModel ID: Doubao-Seedance-1.0-pro Figure 1 Overall evaluation. Left: Text-to-Video; Right: Image-to-Video"
        },
        {
          "chunk_id": "4368ad1653d64bb8b8f2830de1f93734",
          "source": "Seedance 1.0 - 2506.09113v2",
          "chunk_text": ". 21 7.5 Visualization We present several visual outcomes by Seedance 1.0 in Figure 14,15,16. For additional examples, please refer to the official website for an enhanced viewing experience. 8 Conclusion We have introduced Seedance 1.0, a native bilingual video generation foundation model that unifies multiple generation paradigms (such as text-to-video and image-to-video) and excels in instruction following, motion stability, and visual quality. We presented our technical improvements in dataset construction, efficient architecture design with training paradigm, post-training optimization, and inference acceleration, which are integrated effectively to achieve a high-performance model with fast inference. It demonstrates excellent capabilities in handling complex scenarios, multi-shot generation, and long-range temporal coherence, all while delivering fast and photorealistic generation experiences. 22 References [1] artificialanalysis.ai. artificialanalysis"
        },
        {
          "chunk_id": "d3b2c3e23e8e4d91b15587460ea852f8",
          "source": "Seedance 1.0 - 2506.09113v2",
          "chunk_text": "arXiv:2506.09113v2 [cs.CV] 28 Jun 2025 Seedance 1.0: Exploring the Boundaries of Video Generation Models ByteDance Seed Abstract Notable breakthroughs in diffusion modeling have propelled rapid improvements in video generation, yet current foundational model still face critical challenges in simultaneously balancing prompt following, motion plausibility, and visual quality. In this report, we introduce Seedance 1.0 , a high-performance and inference-efficient video foundation generation model that integrates several core technical improvements: (i) multi-source data curation augmented with precision and meaningful video captioning, enabling comprehensive learning across diverse scenarios; (ii) an efficient architecture design with proposed training paradigm, which allows for natively supporting multi-shot generation and jointly learning of both text-to-video and image-to-video tasks"
        },
        {
          "chunk_id": "28e44c0d24424a10bc5c83cecee9b921",
          "source": "Seedance 1.0 - 2506.09113v2",
          "chunk_text": ". To address these limitations, we present Seedance 1.0 , a foundational video generation model with native support bilingual (Chinese/English) video generation and multi-task versatility encompassing text-to-video synthesis and image-guided video generation. Seedance 1.0 integrates four key technical improvements: •Multi-Source Data with Comprehensive Video Captioning. Through multi-stage, multi-perspective curation and dataset balancing, we construct a large-scale high-quality video dataset spanning diverse categories, styles, and sources. This enables a comprehensive learning of rich scenarios, topics, and action dynamics. Our precision video captioning system ensures accurate interpretation of user instructions while enabling fluent generation of complex video narratives. •Efficient Architecture Design. In our design, we decouple spatial and temporal layers with an interleaved multimodal positional encoding"
        },
        {
          "chunk_id": "db78adc1f82e41e68a67b14e9ebc61ef",
          "source": "Seedance 1.0 - 2506.09113v2",
          "chunk_text": ". Seedance 1.0 has participated in both categories. Some notable external competitors include Veo 3, Kling 2.0, Runway Gen4, OpenAI Sora, and Wan 2.1. Seedance 1.0 tops both the text-to-video and image-to-video leaderboards, demonstrating a substantial performance advantage over competing models. In particular, it outperforms the second- and third-best models, Veo 3 and Kling 2.0, by over 100 points in the image-to-video task. Notably, Seedance 1.0 attains state-of-the-art results across both tasks using a single unified model, whereas prior models typically excelled in one domain while underperforming in the other. The subsequent sections provide a detailed analysis of Seedance 1.0’s advantages in each scenario. 7.2 Comprehensive Evaluation Besides overall user preferences, a comprehensive benchmark is equally important for the evaluation of visual generation models, as it enables a more holistic assessment of model capabilities"
        }
      ],
      "metadata": {
        "question_type": "analytical",
        "generated_by": "gpt-4o-mini",
        "top_k": 5,
        "retrieved_chunks_array": [
          {
            "chunk_id": "86b80d8365a047c18c4571b255bb6c48",
            "source": "Seedance 1.0 - 2506.09113v2",
            "chunk_text": ". (iii) carefully-optimized post-training approaches leveraging fine-grained supervised fine-tuning, and video-specific RLHF with multi-dimensional reward mechanisms for comprehensive performance improvements; (iv) excellent model acceleration achieving 10 ×inference speedup through multi- stage distillation strategies and system-level optimizations. Seedance 1.0 can generate a 5-second video at 1080p resolution only with 41.4 seconds (NVIDIA-L20). Compared to state-of-the-art video generation models, Seedance 1.0 stands out with high-quality and fast video generation having superior spatiotemporal fluidity with structural stability, precise instruction adherence in complex multi-subject contexts, native multi-shot narrative coherence with consistent subject representation. Seedance 1.0 is now accessible on Volcano Engineα. Official Page: https://seed.bytedance.com/seedance αModel ID: Doubao-Seedance-1.0-pro Figure 1 Overall evaluation. Left: Text-to-Video; Right: Image-to-Video"
          },
          {
            "chunk_id": "4368ad1653d64bb8b8f2830de1f93734",
            "source": "Seedance 1.0 - 2506.09113v2",
            "chunk_text": ". 21 7.5 Visualization We present several visual outcomes by Seedance 1.0 in Figure 14,15,16. For additional examples, please refer to the official website for an enhanced viewing experience. 8 Conclusion We have introduced Seedance 1.0, a native bilingual video generation foundation model that unifies multiple generation paradigms (such as text-to-video and image-to-video) and excels in instruction following, motion stability, and visual quality. We presented our technical improvements in dataset construction, efficient architecture design with training paradigm, post-training optimization, and inference acceleration, which are integrated effectively to achieve a high-performance model with fast inference. It demonstrates excellent capabilities in handling complex scenarios, multi-shot generation, and long-range temporal coherence, all while delivering fast and photorealistic generation experiences. 22 References [1] artificialanalysis.ai. artificialanalysis"
          },
          {
            "chunk_id": "d3b2c3e23e8e4d91b15587460ea852f8",
            "source": "Seedance 1.0 - 2506.09113v2",
            "chunk_text": "arXiv:2506.09113v2 [cs.CV] 28 Jun 2025 Seedance 1.0: Exploring the Boundaries of Video Generation Models ByteDance Seed Abstract Notable breakthroughs in diffusion modeling have propelled rapid improvements in video generation, yet current foundational model still face critical challenges in simultaneously balancing prompt following, motion plausibility, and visual quality. In this report, we introduce Seedance 1.0 , a high-performance and inference-efficient video foundation generation model that integrates several core technical improvements: (i) multi-source data curation augmented with precision and meaningful video captioning, enabling comprehensive learning across diverse scenarios; (ii) an efficient architecture design with proposed training paradigm, which allows for natively supporting multi-shot generation and jointly learning of both text-to-video and image-to-video tasks"
          },
          {
            "chunk_id": "28e44c0d24424a10bc5c83cecee9b921",
            "source": "Seedance 1.0 - 2506.09113v2",
            "chunk_text": ". To address these limitations, we present Seedance 1.0 , a foundational video generation model with native support bilingual (Chinese/English) video generation and multi-task versatility encompassing text-to-video synthesis and image-guided video generation. Seedance 1.0 integrates four key technical improvements: •Multi-Source Data with Comprehensive Video Captioning. Through multi-stage, multi-perspective curation and dataset balancing, we construct a large-scale high-quality video dataset spanning diverse categories, styles, and sources. This enables a comprehensive learning of rich scenarios, topics, and action dynamics. Our precision video captioning system ensures accurate interpretation of user instructions while enabling fluent generation of complex video narratives. •Efficient Architecture Design. In our design, we decouple spatial and temporal layers with an interleaved multimodal positional encoding"
          },
          {
            "chunk_id": "db78adc1f82e41e68a67b14e9ebc61ef",
            "source": "Seedance 1.0 - 2506.09113v2",
            "chunk_text": ". Seedance 1.0 has participated in both categories. Some notable external competitors include Veo 3, Kling 2.0, Runway Gen4, OpenAI Sora, and Wan 2.1. Seedance 1.0 tops both the text-to-video and image-to-video leaderboards, demonstrating a substantial performance advantage over competing models. In particular, it outperforms the second- and third-best models, Veo 3 and Kling 2.0, by over 100 points in the image-to-video task. Notably, Seedance 1.0 attains state-of-the-art results across both tasks using a single unified model, whereas prior models typically excelled in one domain while underperforming in the other. The subsequent sections provide a detailed analysis of Seedance 1.0’s advantages in each scenario. 7.2 Comprehensive Evaluation Besides overall user preferences, a comprehensive benchmark is equally important for the evaluation of visual generation models, as it enables a more holistic assessment of model capabilities"
          }
        ]
      }
    },
    {
      "question": "What are the main tasks mentioned in the text that have seen progress due to advancements in large multimodal models?",
      "expected_answer": "The main tasks mentioned are text-to-video (T2V) generation and video-to-text (V2T) interpretation.",
      "actual_answer": "The main tasks that have seen progress due to advancements in large multimodal models, as mentioned in the text, include:\n\n1. **Video Understanding**: This involves reasoning with videos, answering questions based on visual content, and generating captions that describe visual scenes.\n\n2. **GUI Navigation and Decision-Making**: Models like CogAgent excel in understanding and navigating graphical user interfaces, leveraging both low and high-resolution video encoders.\n\n3. **Multimodal Perception and Reasoning**: Models such as InternVL3 have enhanced capabilities in multimodal perception and reasoning, extending to tool usage and industrial image analysis.\n\n4. **3D Vision Perception**: This includes tasks related to understanding and interpreting 3D visual data.\n\n5. **OCR (Optical Character Recognition)**: The ability to extract and recognize text from images and videos.\n\n6. **Object Recognition and Linguistic Structure Analysis**: Tasks involving detecting objects and understanding linguistic structures within visual content.\n\n7. **Multimodal Hallucination Detection**: Identifying and mitigating errors where models might generate inaccurate or fabricated information based on multimodal inputs.\n\nThese advancements highlight the enhanced capabilities of large multimodal models in handling complex tasks across various domains involving vision and language.",
      "expected_correct_chunks": "LOVE: Benchmarking and Evaluating Text-to-Video Generation and Video-to-Text Interpretation Jiarui Wang1, Huiyu Duan1,2, Ziheng Jia1, Yu Zhao1, Woo Yi Yang1, Zicheng Zhang1, Zijian Chen1, Juntong Wang1, Yuke Xing1, Guangtao Zhai1,2, Xiongkuo Min1∗ 1Institute of Image Communication and Network Engineering, 2MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University, Shanghai, China Abstract Recent advancements in large multimodal models (LMMs) have driven substantial progress in both text-to-video (T2V) generation and video-to-text (V2T) interpreta- tion tasks. However, current AI-generated videos (AIGVs) still exhibit limitations in terms of perceptual quality and text-video alignment. Therefore, a reliable and scalable automatic model for AIGV evaluation is desirable, which heavily relies on the scale and quality of human annotations",
      "sources": [
        "Love frramework - 2505.12098v1",
        "Love frramework - 2505.12098v1",
        "Love frramework - 2505.12098v1",
        "Love frramework - 2505.12098v1",
        "Love frramework - 2505.12098v1"
      ],
      "chunk_ids": [
        "1d794a0824cb4a8599831cdb4e6f9bf2",
        "0e679b4c63404163b8e6fd1691fc669b",
        "e97664d0a12a4941ab76e9743d25fbca",
        "ec1199435a144986bb285bc0f764e382",
        "89e099083a014b349fb868bf1f46041f"
      ],
      "f1_score": 0.0865979381443299,
      "precision_at_k": 0.054404145077720206,
      "recall_at_k": 0.21212121212121213,
      "mean_cosine_score": 0.5560967917238926,
      "llm_as_judge_score": 0.5,
      "latency": 17.11099410057068,
      "retrieved_chunks_text": ". [4]P. Zhang, X. Dong, B. Wang, Y . Cao, C. Xu, L. Ouyang, Z. Zhao, S. Ding, S. Zhang, H. Duan, W. Zhang, H. Yan, X. Zhang, W. Li, J. Li, K. Chen, C. He, X. Zhang, Y . Qiao, D. Lin, and J. Wang, “Internlm- xcomposer: A vision-language large model for advanced text-image comprehension and composition,” arXiv preprint arXiv:2309.15112 , 2023. [5]J. Ye, H. Xu, H. Liu, A. Hu, M. Yan, Q. Qian, J. Zhang, F. Huang, and J. Zhou, “mplug-owl3: Towards long image-sequence understanding in multi-modal large language models,” in Proceedings of the International Conference on Learning Representations (ICLR) , 2024. [6]F. Li, R. Zhang, H. Zhang, Y . Zhang, B. Li, W. Li, Z. Ma, and C. Li, “Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models,” arXiv preprint arXiv:2407.07895 , 2024. [7]J. Wang, H. Duan, G. Zhai, J. Wang, and X\n. CogAgent excels in GUI tasks like navigation and decision-making. CogAgent’s innova- tive design includes a cross-attention branch to balance high-resolution inputs and computational efficiency. InternVL2.5 [41] demonstrates strong performance in various benchmarks, including multi-discipline reasoning, document and video understanding, and multimodal hallucination detection. The model features enhanced vision encoders, larger dataset sizes, and improved test-time scaling. InternVL3 [41] is an advanced multimodal large language model series that surpasses its predecessor, InternVL 2.5, in multimodal perception and reasoning. It extends its capabilities to tool usage, GUI agents, industrial image analysis, 3D vision perception, and more. Built upon Native Multimodal Pre-Training, InternVL3 integrates language and multimodal training in a single stage, enhancing its performance without requiring additional bridging modules\n. However, these efforts face the following limitations that may ∗Corresponding author Preprint.arXiv:2505.12098v1 [cs.CV] 17 May 2025 Object Counting ColorPositionShape Texture SceneOCR HOI EmotionLinguistic Structure ViewWorld Knowledge Face ImaginationEvent Order Prompt Task -Counting3050(a) Prompt collection (b) T2V Generation 15Annotators per video30T2V models Human 14.3% 8.6% 8.6% 8.3% 6.9%5.9%2.1% 2.3% Style3.0% 3.1% 3.3% 3.5% 3.5% 3.9% 4.0% 4.1% 4.5% 4.8% Motion Direction2.7%2.4% 1550 Prompts9 Databases +AI-genrated 1500 Promptspart ofSources 20TasksComplex Simple 85.7% LOVE Instance LevelComparison of models (d) Model Designis excellent . Score: 68.09 Yes. 60K Videos Generated from 3050 Prompts 120K MOSs Assessed from 2 Perspectives 60K Yes or No Question Answer Pairs on 20 Tasks 3. Deep learning -based VQA methods1.Handcrafted -based models 4. LMM-based models 5\n. These models can reason with videos, such as answering questions based on graphs or maps, and generate captions that describe visual scenes. Llava-one-vision [61] is an open-source large multimodal model (LMM) designed to enhance vision- and-language tasks in single-image, multi-image, and video scenarios. It utilizes a cost-efficient architecture connecting vision encoders with LLMs, demonstrating strong video understanding through task transfer from images. CogAgent [59] is designed to facilitate understanding and navigation of graphical user interfaces (GUIs). It utilizes both low and high-resolution video encoders to recognize small text and page elements. CogAgent excels in GUI tasks like navigation and decision-making. CogAgent’s innova- tive design includes a cross-attention branch to balance high-resolution inputs and computational efficiency\n. LMM-based models with large parameters tend to perform better overall, with enhanced generalization capabilities compared to models with fewer parameters. Some open-source LMMs such as InternVL2.5 (72B) [ 41] and Qwen2.5-VL (72B) [ 57], perform comparably to their closed-source LMMs such as Gemini1.5-pro [ 62] and Claude3.5 [ 63], indicating that parameter size and pretraining techniques play significant roles in achieving higher performance across both perception and correspondence tasks. From Table 14, it is evident that models generally perform well on tasks such as OCR, Object, and Linguistic Structure, where visual features like object recognition and text extraction play a 33 Table 13: Performance comparisons of V2T Interpretation Models on correspondence SRCC. Models Object Color Count. Texture Position HOI Face Emo. Human OCR Scene Style Shape View Know. Ling. Imag",
      "retrieved_chunks": [
        {
          "chunk_id": "1d794a0824cb4a8599831cdb4e6f9bf2",
          "source": "Love frramework - 2505.12098v1",
          "chunk_text": ". [4]P. Zhang, X. Dong, B. Wang, Y . Cao, C. Xu, L. Ouyang, Z. Zhao, S. Ding, S. Zhang, H. Duan, W. Zhang, H. Yan, X. Zhang, W. Li, J. Li, K. Chen, C. He, X. Zhang, Y . Qiao, D. Lin, and J. Wang, “Internlm- xcomposer: A vision-language large model for advanced text-image comprehension and composition,” arXiv preprint arXiv:2309.15112 , 2023. [5]J. Ye, H. Xu, H. Liu, A. Hu, M. Yan, Q. Qian, J. Zhang, F. Huang, and J. Zhou, “mplug-owl3: Towards long image-sequence understanding in multi-modal large language models,” in Proceedings of the International Conference on Learning Representations (ICLR) , 2024. [6]F. Li, R. Zhang, H. Zhang, Y . Zhang, B. Li, W. Li, Z. Ma, and C. Li, “Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models,” arXiv preprint arXiv:2407.07895 , 2024. [7]J. Wang, H. Duan, G. Zhai, J. Wang, and X"
        },
        {
          "chunk_id": "0e679b4c63404163b8e6fd1691fc669b",
          "source": "Love frramework - 2505.12098v1",
          "chunk_text": ". CogAgent excels in GUI tasks like navigation and decision-making. CogAgent’s innova- tive design includes a cross-attention branch to balance high-resolution inputs and computational efficiency. InternVL2.5 [41] demonstrates strong performance in various benchmarks, including multi-discipline reasoning, document and video understanding, and multimodal hallucination detection. The model features enhanced vision encoders, larger dataset sizes, and improved test-time scaling. InternVL3 [41] is an advanced multimodal large language model series that surpasses its predecessor, InternVL 2.5, in multimodal perception and reasoning. It extends its capabilities to tool usage, GUI agents, industrial image analysis, 3D vision perception, and more. Built upon Native Multimodal Pre-Training, InternVL3 integrates language and multimodal training in a single stage, enhancing its performance without requiring additional bridging modules"
        },
        {
          "chunk_id": "e97664d0a12a4941ab76e9743d25fbca",
          "source": "Love frramework - 2505.12098v1",
          "chunk_text": ". However, these efforts face the following limitations that may ∗Corresponding author Preprint.arXiv:2505.12098v1 [cs.CV] 17 May 2025 Object Counting ColorPositionShape Texture SceneOCR HOI EmotionLinguistic Structure ViewWorld Knowledge Face ImaginationEvent Order Prompt Task -Counting3050(a) Prompt collection (b) T2V Generation 15Annotators per video30T2V models Human 14.3% 8.6% 8.6% 8.3% 6.9%5.9%2.1% 2.3% Style3.0% 3.1% 3.3% 3.5% 3.5% 3.9% 4.0% 4.1% 4.5% 4.8% Motion Direction2.7%2.4% 1550 Prompts9 Databases +AI-genrated 1500 Promptspart ofSources 20TasksComplex Simple 85.7% LOVE Instance LevelComparison of models (d) Model Designis excellent . Score: 68.09 Yes. 60K Videos Generated from 3050 Prompts 120K MOSs Assessed from 2 Perspectives 60K Yes or No Question Answer Pairs on 20 Tasks 3. Deep learning -based VQA methods1.Handcrafted -based models 4. LMM-based models 5"
        },
        {
          "chunk_id": "ec1199435a144986bb285bc0f764e382",
          "source": "Love frramework - 2505.12098v1",
          "chunk_text": ". These models can reason with videos, such as answering questions based on graphs or maps, and generate captions that describe visual scenes. Llava-one-vision [61] is an open-source large multimodal model (LMM) designed to enhance vision- and-language tasks in single-image, multi-image, and video scenarios. It utilizes a cost-efficient architecture connecting vision encoders with LLMs, demonstrating strong video understanding through task transfer from images. CogAgent [59] is designed to facilitate understanding and navigation of graphical user interfaces (GUIs). It utilizes both low and high-resolution video encoders to recognize small text and page elements. CogAgent excels in GUI tasks like navigation and decision-making. CogAgent’s innova- tive design includes a cross-attention branch to balance high-resolution inputs and computational efficiency"
        },
        {
          "chunk_id": "89e099083a014b349fb868bf1f46041f",
          "source": "Love frramework - 2505.12098v1",
          "chunk_text": ". LMM-based models with large parameters tend to perform better overall, with enhanced generalization capabilities compared to models with fewer parameters. Some open-source LMMs such as InternVL2.5 (72B) [ 41] and Qwen2.5-VL (72B) [ 57], perform comparably to their closed-source LMMs such as Gemini1.5-pro [ 62] and Claude3.5 [ 63], indicating that parameter size and pretraining techniques play significant roles in achieving higher performance across both perception and correspondence tasks. From Table 14, it is evident that models generally perform well on tasks such as OCR, Object, and Linguistic Structure, where visual features like object recognition and text extraction play a 33 Table 13: Performance comparisons of V2T Interpretation Models on correspondence SRCC. Models Object Color Count. Texture Position HOI Face Emo. Human OCR Scene Style Shape View Know. Ling. Imag"
        }
      ],
      "metadata": {
        "question_type": "factual",
        "generated_by": "gpt-4o-mini",
        "top_k": 5,
        "retrieved_chunks_array": [
          {
            "chunk_id": "1d794a0824cb4a8599831cdb4e6f9bf2",
            "source": "Love frramework - 2505.12098v1",
            "chunk_text": ". [4]P. Zhang, X. Dong, B. Wang, Y . Cao, C. Xu, L. Ouyang, Z. Zhao, S. Ding, S. Zhang, H. Duan, W. Zhang, H. Yan, X. Zhang, W. Li, J. Li, K. Chen, C. He, X. Zhang, Y . Qiao, D. Lin, and J. Wang, “Internlm- xcomposer: A vision-language large model for advanced text-image comprehension and composition,” arXiv preprint arXiv:2309.15112 , 2023. [5]J. Ye, H. Xu, H. Liu, A. Hu, M. Yan, Q. Qian, J. Zhang, F. Huang, and J. Zhou, “mplug-owl3: Towards long image-sequence understanding in multi-modal large language models,” in Proceedings of the International Conference on Learning Representations (ICLR) , 2024. [6]F. Li, R. Zhang, H. Zhang, Y . Zhang, B. Li, W. Li, Z. Ma, and C. Li, “Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models,” arXiv preprint arXiv:2407.07895 , 2024. [7]J. Wang, H. Duan, G. Zhai, J. Wang, and X"
          },
          {
            "chunk_id": "0e679b4c63404163b8e6fd1691fc669b",
            "source": "Love frramework - 2505.12098v1",
            "chunk_text": ". CogAgent excels in GUI tasks like navigation and decision-making. CogAgent’s innova- tive design includes a cross-attention branch to balance high-resolution inputs and computational efficiency. InternVL2.5 [41] demonstrates strong performance in various benchmarks, including multi-discipline reasoning, document and video understanding, and multimodal hallucination detection. The model features enhanced vision encoders, larger dataset sizes, and improved test-time scaling. InternVL3 [41] is an advanced multimodal large language model series that surpasses its predecessor, InternVL 2.5, in multimodal perception and reasoning. It extends its capabilities to tool usage, GUI agents, industrial image analysis, 3D vision perception, and more. Built upon Native Multimodal Pre-Training, InternVL3 integrates language and multimodal training in a single stage, enhancing its performance without requiring additional bridging modules"
          },
          {
            "chunk_id": "e97664d0a12a4941ab76e9743d25fbca",
            "source": "Love frramework - 2505.12098v1",
            "chunk_text": ". However, these efforts face the following limitations that may ∗Corresponding author Preprint.arXiv:2505.12098v1 [cs.CV] 17 May 2025 Object Counting ColorPositionShape Texture SceneOCR HOI EmotionLinguistic Structure ViewWorld Knowledge Face ImaginationEvent Order Prompt Task -Counting3050(a) Prompt collection (b) T2V Generation 15Annotators per video30T2V models Human 14.3% 8.6% 8.6% 8.3% 6.9%5.9%2.1% 2.3% Style3.0% 3.1% 3.3% 3.5% 3.5% 3.9% 4.0% 4.1% 4.5% 4.8% Motion Direction2.7%2.4% 1550 Prompts9 Databases +AI-genrated 1500 Promptspart ofSources 20TasksComplex Simple 85.7% LOVE Instance LevelComparison of models (d) Model Designis excellent . Score: 68.09 Yes. 60K Videos Generated from 3050 Prompts 120K MOSs Assessed from 2 Perspectives 60K Yes or No Question Answer Pairs on 20 Tasks 3. Deep learning -based VQA methods1.Handcrafted -based models 4. LMM-based models 5"
          },
          {
            "chunk_id": "ec1199435a144986bb285bc0f764e382",
            "source": "Love frramework - 2505.12098v1",
            "chunk_text": ". These models can reason with videos, such as answering questions based on graphs or maps, and generate captions that describe visual scenes. Llava-one-vision [61] is an open-source large multimodal model (LMM) designed to enhance vision- and-language tasks in single-image, multi-image, and video scenarios. It utilizes a cost-efficient architecture connecting vision encoders with LLMs, demonstrating strong video understanding through task transfer from images. CogAgent [59] is designed to facilitate understanding and navigation of graphical user interfaces (GUIs). It utilizes both low and high-resolution video encoders to recognize small text and page elements. CogAgent excels in GUI tasks like navigation and decision-making. CogAgent’s innova- tive design includes a cross-attention branch to balance high-resolution inputs and computational efficiency"
          },
          {
            "chunk_id": "89e099083a014b349fb868bf1f46041f",
            "source": "Love frramework - 2505.12098v1",
            "chunk_text": ". LMM-based models with large parameters tend to perform better overall, with enhanced generalization capabilities compared to models with fewer parameters. Some open-source LMMs such as InternVL2.5 (72B) [ 41] and Qwen2.5-VL (72B) [ 57], perform comparably to their closed-source LMMs such as Gemini1.5-pro [ 62] and Claude3.5 [ 63], indicating that parameter size and pretraining techniques play significant roles in achieving higher performance across both perception and correspondence tasks. From Table 14, it is evident that models generally perform well on tasks such as OCR, Object, and Linguistic Structure, where visual features like object recognition and text extraction play a 33 Table 13: Performance comparisons of V2T Interpretation Models on correspondence SRCC. Models Object Color Count. Texture Position HOI Face Emo. Human OCR Scene Style Shape View Know. Ling. Imag"
          }
        ]
      }
    },
    {
      "question": "What is AIGVE-60K, and what are its key features?",
      "expected_answer": "AIGVE-60K is a comprehensive dataset and benchmark for AI-Generated Video Evaluation. Its key features include 3,050 extensive prompts across 20 fine-grained task dimensions, the largest human annotations with 120K mean-opinion scores (MOSs) and 60K question-answering (QA) pairs annotated on 58,500 videos from 30 T2V models, and bidirectional benchmarking for both T2V generation and V2T interpretation capabilities.",
      "actual_answer": "AIGVE-60K is a comprehensive dataset and benchmark designed for the evaluation of AI-Generated Videos (AIGV). It is notable for being the largest dataset of its kind, facilitating the evaluation of video generation and interpretation models. Here are its key features:\n\n1. **Comprehensive Tasks**: AIGVE-60K includes 3,050 extensive prompts across 20 fine-grained task dimensions. These tasks are designed to evaluate complex scenarios by combining simpler task components, such as motion direction, event order, and counting.\n\n2. **Extensive Human Annotations**: The dataset includes the largest set of human annotations for AIGV evaluation, featuring 120,000 mean-opinion scores (MOSs) and 60,000 question-answering (QA) pairs. These annotations are provided for 58,500 videos generated from 30 text-to-video (T2V) models.\n\n3. **Bidirectional Benchmarking**: AIGVE-60K supports the benchmarking and evaluation of both T2V generation capabilities and video-to-text (V2T) interpretation abilities. It assesses models from multiple dimensions, including perceptual preference, text-video correspondence, and task-specific accuracy at both instance and model levels.\n\n4. **Diverse Data Collection**: Prompts for AIGVE-60K are sourced from nine existing open-domain text-video pair datasets and are refined to ensure clarity and diversity.\n\n5. **LOVE Metric**: Based on AIGVE-60K, the LOVE (LMM-based metric for AIGV Evaluation) is proposed. This metric evaluates AIGVs across multiple dimensions, leveraging instruction tuning and LoRA adaptation to achieve perceptual quality evaluation and T2V correspondence attribution.\n\nThese features make AIGVE-60K a critical resource for advancing the development and evaluation of AI-generated video models.",
      "expected_correct_chunks": ". Therefore, a reliable and scalable automatic model for AIGV evaluation is desirable, which heavily relies on the scale and quality of human annotations. To this end, we present AIGVE-60K , a comprehensive dataset and benchmark for AI-Generated Video Evaluation, which features (i) comprehensive tasks , encompassing 3,050 extensive prompts across 20 fine-grained task dimensions, (ii) the largest human annotations , including 120K mean-opinion scores (MOSs) and 60K question-answering (QA) pairs an- notated on 58,500 videos generated from 30 T2V models, and (iii) bidirectional benchmarking and evaluating for both T2V generation and V2T interpretation capabilities. Based on AIGVE-60K, we propose LOVE , aLMM-based metric for AIG VEvaluation from multiple dimensions including perceptual preference, text-video correspondence, and task-specific accuracy in terms of both instance level and model level",
      "sources": [
        "Love frramework - 2505.12098v1",
        "Love frramework - 2505.12098v1",
        "Love frramework - 2505.12098v1",
        "Love frramework - 2505.12098v1",
        "Love frramework - 2505.12098v1"
      ],
      "chunk_ids": [
        "c979c565ff244863941fe02b4cd284ad",
        "8505ee06eac34d7aa058ea6c718aba41",
        "382c84f5666f478ea958cb78e5cd5ecf",
        "f57d799fe10549e8a54a21da564b158b",
        "c570c3a9a4274951a8152948d65e30a0"
      ],
      "f1_score": 0.4987277353689567,
      "precision_at_k": 0.33220338983050846,
      "recall_at_k": 1.0,
      "mean_cosine_score": 0.49651020755854514,
      "llm_as_judge_score": 1.0,
      "latency": 22.51060199737549,
      "retrieved_chunks_text": ". Therefore, a reliable and scalable automatic model for AIGV evaluation is desirable, which heavily relies on the scale and quality of human annotations. To this end, we present AIGVE-60K , a comprehensive dataset and benchmark for AI-Generated Video Evaluation, which features (i) comprehensive tasks , encompassing 3,050 extensive prompts across 20 fine-grained task dimensions, (ii) the largest human annotations , including 120K mean-opinion scores (MOSs) and 60K question-answering (QA) pairs an- notated on 58,500 videos generated from 30 T2V models, and (iii) bidirectional benchmarking and evaluating for both T2V generation and V2T interpretation capabilities. Based on AIGVE-60K, we propose LOVE , aLMM-based metric for AIG VEvaluation from multiple dimensions including perceptual preference, text-video correspondence, and task-specific accuracy in terms of both instance level and model level\n. . . . . . . . . . . . . . . . . . . . 23 E.3 Annotation Interface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 E.4 Annotation Management . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 F More Analysis of AIGVE-60K Database 26 F.1 MOS Distribution across 20 Challenges . . . . . . . . . . . . . . . . . . . . . . 26 F.2 T2V Model Performance across 20 Challenges . . . . . . . . . . . . . . . . . . 26 G Details of Loss Function 28 H Implemention Details 29 H.1 Detailed Information of Evaluation Criteria . . . . . . . . . . . . . . . . . . . . 29 H.2 Detailed Information of Evaluation Methods . . . . . . . . . . . . . . . . . . . 29 H.3 Question design for LLM-based models . . . . . . . . . . . . . . . . . . . . . . 32 I More Results Comparisons 33 A Overview In this Appendix, we provide additional details on the Ethical Discussions, data collection, methodol- ogy, experiments, and results discussed in the main paper\n. 3.1 Data Collection Prompts of the AIGVE-60K are primarily sourced from 9 existing open-domain text-video pair datasets and some are refined using DeepSeek R1 [ 37] to expand and modify them, ensuring clarity and diversity. Our prompt design focuses on 20 different tasks as shown in Figure 1(a). The complex tasks are designed by combining simpler task components, such as motion direction, event order, and counting, into more complex challenges. In total, we collect 3,050 prompts, each corresponding to a specific task. To generate the AIGVs, we utilize 30 of the latest T2V models, as shown in Figure 1(b). We leverage open-source website APIs or the default weights of these models to generate videos. For the training set, we employ 2,750 distinct prompts, each processed by 18 open-source models. The test set consists of 300 unique prompts generated using all 30 models\n. 6 Table 2: Performance benchmark on AIGVE-60K. Overall-Averaged indicates that we average the MOS of the two dimensions as the overall video quality score. ♠Conventional handcrafted metrics, ♢VBench metrics, ♣deep learning-based VQA models, ♡vision-language pre-training models, ⋆open-source LMM-based models, and △close-source LMM-based models. ♦*Refers to fine-tuned models. The best results are marked in RED and the second-best in BLUE\n. Third, the efficacy of the temporal features is validated by experiments (5) and (6). 6 Conclusion In this paper, we introduce AIGVE-60K , the largest AIGV evaluation dataset to date, consisting of 58,500 videos generated by 30 T2V models using 3,050 prompts across 20 task-specific challenges and 2.6M subjective ratings from the perception, text-video correspondence, and task-specific accuracy, respectively. Based on AIGVE-60K, we benchmark and evaluate both the generation ability of T2V models and the V2T interpretation ability of LMMs. We also propose LOVE , a LMM-based evaluation model that leverages instruction tuning and LoRA adaptation to achieve AIGV perceptual quality evaluation and T2V correspondence attribution in terms of both model-level and instance level",
      "retrieved_chunks": [
        {
          "chunk_id": "c979c565ff244863941fe02b4cd284ad",
          "source": "Love frramework - 2505.12098v1",
          "chunk_text": ". Therefore, a reliable and scalable automatic model for AIGV evaluation is desirable, which heavily relies on the scale and quality of human annotations. To this end, we present AIGVE-60K , a comprehensive dataset and benchmark for AI-Generated Video Evaluation, which features (i) comprehensive tasks , encompassing 3,050 extensive prompts across 20 fine-grained task dimensions, (ii) the largest human annotations , including 120K mean-opinion scores (MOSs) and 60K question-answering (QA) pairs an- notated on 58,500 videos generated from 30 T2V models, and (iii) bidirectional benchmarking and evaluating for both T2V generation and V2T interpretation capabilities. Based on AIGVE-60K, we propose LOVE , aLMM-based metric for AIG VEvaluation from multiple dimensions including perceptual preference, text-video correspondence, and task-specific accuracy in terms of both instance level and model level"
        },
        {
          "chunk_id": "8505ee06eac34d7aa058ea6c718aba41",
          "source": "Love frramework - 2505.12098v1",
          "chunk_text": ". . . . . . . . . . . . . . . . . . . . 23 E.3 Annotation Interface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 E.4 Annotation Management . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 F More Analysis of AIGVE-60K Database 26 F.1 MOS Distribution across 20 Challenges . . . . . . . . . . . . . . . . . . . . . . 26 F.2 T2V Model Performance across 20 Challenges . . . . . . . . . . . . . . . . . . 26 G Details of Loss Function 28 H Implemention Details 29 H.1 Detailed Information of Evaluation Criteria . . . . . . . . . . . . . . . . . . . . 29 H.2 Detailed Information of Evaluation Methods . . . . . . . . . . . . . . . . . . . 29 H.3 Question design for LLM-based models . . . . . . . . . . . . . . . . . . . . . . 32 I More Results Comparisons 33 A Overview In this Appendix, we provide additional details on the Ethical Discussions, data collection, methodol- ogy, experiments, and results discussed in the main paper"
        },
        {
          "chunk_id": "382c84f5666f478ea958cb78e5cd5ecf",
          "source": "Love frramework - 2505.12098v1",
          "chunk_text": ". 3.1 Data Collection Prompts of the AIGVE-60K are primarily sourced from 9 existing open-domain text-video pair datasets and some are refined using DeepSeek R1 [ 37] to expand and modify them, ensuring clarity and diversity. Our prompt design focuses on 20 different tasks as shown in Figure 1(a). The complex tasks are designed by combining simpler task components, such as motion direction, event order, and counting, into more complex challenges. In total, we collect 3,050 prompts, each corresponding to a specific task. To generate the AIGVs, we utilize 30 of the latest T2V models, as shown in Figure 1(b). We leverage open-source website APIs or the default weights of these models to generate videos. For the training set, we employ 2,750 distinct prompts, each processed by 18 open-source models. The test set consists of 300 unique prompts generated using all 30 models"
        },
        {
          "chunk_id": "f57d799fe10549e8a54a21da564b158b",
          "source": "Love frramework - 2505.12098v1",
          "chunk_text": ". 6 Table 2: Performance benchmark on AIGVE-60K. Overall-Averaged indicates that we average the MOS of the two dimensions as the overall video quality score. ♠Conventional handcrafted metrics, ♢VBench metrics, ♣deep learning-based VQA models, ♡vision-language pre-training models, ⋆open-source LMM-based models, and △close-source LMM-based models. ♦*Refers to fine-tuned models. The best results are marked in RED and the second-best in BLUE"
        },
        {
          "chunk_id": "c570c3a9a4274951a8152948d65e30a0",
          "source": "Love frramework - 2505.12098v1",
          "chunk_text": ". Third, the efficacy of the temporal features is validated by experiments (5) and (6). 6 Conclusion In this paper, we introduce AIGVE-60K , the largest AIGV evaluation dataset to date, consisting of 58,500 videos generated by 30 T2V models using 3,050 prompts across 20 task-specific challenges and 2.6M subjective ratings from the perception, text-video correspondence, and task-specific accuracy, respectively. Based on AIGVE-60K, we benchmark and evaluate both the generation ability of T2V models and the V2T interpretation ability of LMMs. We also propose LOVE , a LMM-based evaluation model that leverages instruction tuning and LoRA adaptation to achieve AIGV perceptual quality evaluation and T2V correspondence attribution in terms of both model-level and instance level"
        }
      ],
      "metadata": {
        "question_type": "factual",
        "generated_by": "gpt-4o-mini",
        "top_k": 5,
        "retrieved_chunks_array": [
          {
            "chunk_id": "c979c565ff244863941fe02b4cd284ad",
            "source": "Love frramework - 2505.12098v1",
            "chunk_text": ". Therefore, a reliable and scalable automatic model for AIGV evaluation is desirable, which heavily relies on the scale and quality of human annotations. To this end, we present AIGVE-60K , a comprehensive dataset and benchmark for AI-Generated Video Evaluation, which features (i) comprehensive tasks , encompassing 3,050 extensive prompts across 20 fine-grained task dimensions, (ii) the largest human annotations , including 120K mean-opinion scores (MOSs) and 60K question-answering (QA) pairs an- notated on 58,500 videos generated from 30 T2V models, and (iii) bidirectional benchmarking and evaluating for both T2V generation and V2T interpretation capabilities. Based on AIGVE-60K, we propose LOVE , aLMM-based metric for AIG VEvaluation from multiple dimensions including perceptual preference, text-video correspondence, and task-specific accuracy in terms of both instance level and model level"
          },
          {
            "chunk_id": "8505ee06eac34d7aa058ea6c718aba41",
            "source": "Love frramework - 2505.12098v1",
            "chunk_text": ". . . . . . . . . . . . . . . . . . . . 23 E.3 Annotation Interface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 E.4 Annotation Management . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 F More Analysis of AIGVE-60K Database 26 F.1 MOS Distribution across 20 Challenges . . . . . . . . . . . . . . . . . . . . . . 26 F.2 T2V Model Performance across 20 Challenges . . . . . . . . . . . . . . . . . . 26 G Details of Loss Function 28 H Implemention Details 29 H.1 Detailed Information of Evaluation Criteria . . . . . . . . . . . . . . . . . . . . 29 H.2 Detailed Information of Evaluation Methods . . . . . . . . . . . . . . . . . . . 29 H.3 Question design for LLM-based models . . . . . . . . . . . . . . . . . . . . . . 32 I More Results Comparisons 33 A Overview In this Appendix, we provide additional details on the Ethical Discussions, data collection, methodol- ogy, experiments, and results discussed in the main paper"
          },
          {
            "chunk_id": "382c84f5666f478ea958cb78e5cd5ecf",
            "source": "Love frramework - 2505.12098v1",
            "chunk_text": ". 3.1 Data Collection Prompts of the AIGVE-60K are primarily sourced from 9 existing open-domain text-video pair datasets and some are refined using DeepSeek R1 [ 37] to expand and modify them, ensuring clarity and diversity. Our prompt design focuses on 20 different tasks as shown in Figure 1(a). The complex tasks are designed by combining simpler task components, such as motion direction, event order, and counting, into more complex challenges. In total, we collect 3,050 prompts, each corresponding to a specific task. To generate the AIGVs, we utilize 30 of the latest T2V models, as shown in Figure 1(b). We leverage open-source website APIs or the default weights of these models to generate videos. For the training set, we employ 2,750 distinct prompts, each processed by 18 open-source models. The test set consists of 300 unique prompts generated using all 30 models"
          },
          {
            "chunk_id": "f57d799fe10549e8a54a21da564b158b",
            "source": "Love frramework - 2505.12098v1",
            "chunk_text": ". 6 Table 2: Performance benchmark on AIGVE-60K. Overall-Averaged indicates that we average the MOS of the two dimensions as the overall video quality score. ♠Conventional handcrafted metrics, ♢VBench metrics, ♣deep learning-based VQA models, ♡vision-language pre-training models, ⋆open-source LMM-based models, and △close-source LMM-based models. ♦*Refers to fine-tuned models. The best results are marked in RED and the second-best in BLUE"
          },
          {
            "chunk_id": "c570c3a9a4274951a8152948d65e30a0",
            "source": "Love frramework - 2505.12098v1",
            "chunk_text": ". Third, the efficacy of the temporal features is validated by experiments (5) and (6). 6 Conclusion In this paper, we introduce AIGVE-60K , the largest AIGV evaluation dataset to date, consisting of 58,500 videos generated by 30 T2V models using 3,050 prompts across 20 task-specific challenges and 2.6M subjective ratings from the perception, text-video correspondence, and task-specific accuracy, respectively. Based on AIGVE-60K, we benchmark and evaluate both the generation ability of T2V models and the V2T interpretation ability of LMMs. We also propose LOVE , a LMM-based evaluation model that leverages instruction tuning and LoRA adaptation to achieve AIGV perceptual quality evaluation and T2V correspondence attribution in terms of both model-level and instance level"
          }
        ]
      }
    },
    {
      "question": "What is the primary purpose of the GENIE model introduced in the paper?",
      "expected_answer": "The primary purpose of the GENIE model is to serve as a novel diffusion language model pre-training framework for text generation, which generates text by transforming a random noise sequence into a coherent text sequence.",
      "actual_answer": "The primary purpose of the GENIE model is to serve as a powerful non-autoregressive (NAR) model for text generation. It aims to produce fluent and coherent texts for various applications by leveraging a diffusion language model approach. GENIE adopts a sequence-to-sequence framework with a bidirectional encoder and a cross-attention diffusion model to generate high-quality text sequences. This approach allows it to generate text in parallel at each denoising step, enhancing its ability to produce diverse and coherent text samples. GENIE is evaluated on several popular text generation benchmarks, demonstrating competitive performance with state-of-the-art autoregressive models and achieving significant improvements in diversity metrics.",
      "expected_correct_chunks": "Text Generation with Diffusion Language Models: A Pre-training Approach with Continuous Paragraph Denoise Zhenghao Lin1 2Yeyun Gong3Yelong Shen4Tong Wu5 2Zhihao Fan6 2 Chen Lin1Nan Duan3Weizhu Chen4 Abstract In this paper, we introduce a novel d Iffusion language mod El pre-training framework for text generation, which we call GENIE .GENIE is a large-scale pretrained diffusion language model that consists of an encoder and a diffusion-based decoder, which can generate text by gradually transforming a random noise sequence into a coherent text sequence. To pre-train GENIE on a large-scale language corpus, we design a new continuous paragraph denoise objective, which encourages the diffusion-decoder to reconstruct a clean text paragraph from a corrupted version, while preserving the semantic and syntactic coherence. We evaluate GENIE on four downstream text generation benchmarks, namely XSUM,CNN/D AILY MAIL,GIGA- WORD , and COMMON GEN",
      "sources": [
        "Text diffusion - 2212.11685v2",
        "Text diffusion - 2212.11685v2",
        "Text diffusion - 2212.11685v2",
        "Text diffusion - 2212.11685v2",
        "Text diffusion - 2212.11685v2"
      ],
      "chunk_ids": [
        "36b636a9f90b4e2c92687fe34ecde216",
        "06d0f814bea54edea7f4a6db2cdb635e",
        "79988d53076a456e9850604a7cf7a162",
        "e29068c2161f4848aef9465e04553e89",
        "6113fe37f3ca4637895495f9a2abb6d2"
      ],
      "f1_score": 0.20434782608695656,
      "precision_at_k": 0.1298342541436464,
      "recall_at_k": 0.47959183673469385,
      "mean_cosine_score": 0.5296436827698845,
      "llm_as_judge_score": 0.9,
      "latency": 10.383379936218262,
      "retrieved_chunks_text": ". It is worth noting that GENIE also belongs to the Semi-NAR model. The second group is AR model, the model of encoder- decoder structure including LSTM (Greff et al., 2017), Transformer (Vaswani et al., 2017a), bRNN-CopyNet (Gu et al., 2016a), Trans-CopyNet (Lin et al., 2019),MeanPooling-CopyNet (Lin et al., 2019) without pre- training, and strong baselines MASS (Song et al., 2019), BART (Lewis et al., 2019), T5 (Raffel et al., 2020b), BANG (Qi et al., 2021), and ProphetNet (Qi et al., 2020) with large scale pre-training. For large scale pre-training models mentioned above, we select the base version of the model, which is equivalent to the total number of GENIE parameters. 4.4. Main Results We present the results of GENIE and the baselines on XSUM,CNN/D AILY MAIL,GIGAWORD ,COMMON GENin Table 1, Table 2, and Table 3. Our results demonstrate that the pre-trained GENIE is a powerful NAR model for text generation\n. We evaluate GENIE on four downstream text generation benchmarks, namely XSUM,CNN/D AILY MAIL,GIGA- WORD , and COMMON GEN. Our experimental results show that GENIE achieves comparable performance with the state-of-the-art autore- gressive models on these benchmarks, and generates more diverse text samples. The code and models of GENIE are available athttps://github.com/microsoft/ ProphetNet/tree/master/GENIE . 1. Introduction Text generation is a crucial task in natural language process- ing, which aims to produce ﬂuent and coherent texts for var- ious applications. Previous text generation methods mainly relied on recurrent neural networks (RNNs) (Pawade et al., 2018; Song et al., 2018; Gu et al., 2016a; Qi et al., 2021), which generate texts sequentially from left to right. How- ever, RNNs suffer from issues such as long-term dependency and exposure bias\n. Common Sense Generation Common sense generation tasks require the model have the ability of generative com- monsense reasoning. Speciﬁcally, given a series of common sense concepts, the model needs to generate coherent state- ments based on these concepts that adhere to real-world scenarios. We select the widely used dataset COMMON - GEN(Lin et al., 2019) to evaluate whether GENIE has good creativity and reasoning ability in natural language generat- ing. In the ﬁne-tuning phase, we set the learning rate to 1e-4 and train 10k steps in total. Finally, we randomly sampled 10 gaussian noises and selected the best sample as the ﬁnal result. Referring to the previous work (Lin et al., 2019), we Text Generation with Diffusion Language Models: A Pre-training Approach with Continuous Paragraph Denoise Table 1. Results of Semi-NAR, NAR and AR on XSUM. Index OVERALL represents the average value of ROUGE-1 ,ROUGE-2 and ROUGE-L . It should be noted that GENIE belongs to Semi-NAR\n. We evaluate GENIE on four popular text generation bench- marks: XSum (Narayan et al., 2018), CNN/DailyMail (Her- mann et al., 2015), Gigaword (Rush et al., 2015), and Com- monGen (Lin et al., 2019). The experimental results demon- strate that GENIE achieves competitive performance witharXiv:2212.11685v2 [cs.CL] 17 Feb 2023 Text Generation with Diffusion Language Models: A Pre-training Approach with Continuous Paragraph Denoise Transformer-based AR methods, and that the proposed pre- training method can effectively improve the performance. We notice that GENIE has achieved signiﬁcant improve- ments in diversity metric. To evaluate the multiple outputs of the generation model, we design an automatic annotation method based on large language model. We also conduct ablation studies to analyze the impact of the diffusion steps and pre-training steps\n. Model GENIE is the proposed diffusion language model for pre- training, it adopts the sequence-to-sequence framework as illustrated in Figure 1. GENIE could generate a high- quality text sequence ygiven a source text s, such as pro- ducing y:Messi’s performance froms:In the World Cup 2022, [MASK] won people’s praise. . To achieve this, GENIE leverages two components: a bidirectional encoder model and a cross-attention diffusion model. The encoder model encodes the source text sinto a set of hidden vec- torsHs=Encoder (s), which indicates the distributed representation of s. The diffusion model takes Hsand a Gaussian noise as inputs, and iteratively reﬁnes the data by applying a sequence of denoising operations. In contrast to the traditional autoregressive text generation paradigm, which generates one token at a time, the diffusion model in GENIE outputs the sequence of embeddings in parallel at each denoising step, making GENIE a non-autoregressive generation (NAR) model",
      "retrieved_chunks": [
        {
          "chunk_id": "36b636a9f90b4e2c92687fe34ecde216",
          "source": "Text diffusion - 2212.11685v2",
          "chunk_text": ". It is worth noting that GENIE also belongs to the Semi-NAR model. The second group is AR model, the model of encoder- decoder structure including LSTM (Greff et al., 2017), Transformer (Vaswani et al., 2017a), bRNN-CopyNet (Gu et al., 2016a), Trans-CopyNet (Lin et al., 2019),MeanPooling-CopyNet (Lin et al., 2019) without pre- training, and strong baselines MASS (Song et al., 2019), BART (Lewis et al., 2019), T5 (Raffel et al., 2020b), BANG (Qi et al., 2021), and ProphetNet (Qi et al., 2020) with large scale pre-training. For large scale pre-training models mentioned above, we select the base version of the model, which is equivalent to the total number of GENIE parameters. 4.4. Main Results We present the results of GENIE and the baselines on XSUM,CNN/D AILY MAIL,GIGAWORD ,COMMON GENin Table 1, Table 2, and Table 3. Our results demonstrate that the pre-trained GENIE is a powerful NAR model for text generation"
        },
        {
          "chunk_id": "06d0f814bea54edea7f4a6db2cdb635e",
          "source": "Text diffusion - 2212.11685v2",
          "chunk_text": ". We evaluate GENIE on four downstream text generation benchmarks, namely XSUM,CNN/D AILY MAIL,GIGA- WORD , and COMMON GEN. Our experimental results show that GENIE achieves comparable performance with the state-of-the-art autore- gressive models on these benchmarks, and generates more diverse text samples. The code and models of GENIE are available athttps://github.com/microsoft/ ProphetNet/tree/master/GENIE . 1. Introduction Text generation is a crucial task in natural language process- ing, which aims to produce ﬂuent and coherent texts for var- ious applications. Previous text generation methods mainly relied on recurrent neural networks (RNNs) (Pawade et al., 2018; Song et al., 2018; Gu et al., 2016a; Qi et al., 2021), which generate texts sequentially from left to right. How- ever, RNNs suffer from issues such as long-term dependency and exposure bias"
        },
        {
          "chunk_id": "79988d53076a456e9850604a7cf7a162",
          "source": "Text diffusion - 2212.11685v2",
          "chunk_text": ". Common Sense Generation Common sense generation tasks require the model have the ability of generative com- monsense reasoning. Speciﬁcally, given a series of common sense concepts, the model needs to generate coherent state- ments based on these concepts that adhere to real-world scenarios. We select the widely used dataset COMMON - GEN(Lin et al., 2019) to evaluate whether GENIE has good creativity and reasoning ability in natural language generat- ing. In the ﬁne-tuning phase, we set the learning rate to 1e-4 and train 10k steps in total. Finally, we randomly sampled 10 gaussian noises and selected the best sample as the ﬁnal result. Referring to the previous work (Lin et al., 2019), we Text Generation with Diffusion Language Models: A Pre-training Approach with Continuous Paragraph Denoise Table 1. Results of Semi-NAR, NAR and AR on XSUM. Index OVERALL represents the average value of ROUGE-1 ,ROUGE-2 and ROUGE-L . It should be noted that GENIE belongs to Semi-NAR"
        },
        {
          "chunk_id": "e29068c2161f4848aef9465e04553e89",
          "source": "Text diffusion - 2212.11685v2",
          "chunk_text": ". We evaluate GENIE on four popular text generation bench- marks: XSum (Narayan et al., 2018), CNN/DailyMail (Her- mann et al., 2015), Gigaword (Rush et al., 2015), and Com- monGen (Lin et al., 2019). The experimental results demon- strate that GENIE achieves competitive performance witharXiv:2212.11685v2 [cs.CL] 17 Feb 2023 Text Generation with Diffusion Language Models: A Pre-training Approach with Continuous Paragraph Denoise Transformer-based AR methods, and that the proposed pre- training method can effectively improve the performance. We notice that GENIE has achieved signiﬁcant improve- ments in diversity metric. To evaluate the multiple outputs of the generation model, we design an automatic annotation method based on large language model. We also conduct ablation studies to analyze the impact of the diffusion steps and pre-training steps"
        },
        {
          "chunk_id": "6113fe37f3ca4637895495f9a2abb6d2",
          "source": "Text diffusion - 2212.11685v2",
          "chunk_text": ". Model GENIE is the proposed diffusion language model for pre- training, it adopts the sequence-to-sequence framework as illustrated in Figure 1. GENIE could generate a high- quality text sequence ygiven a source text s, such as pro- ducing y:Messi’s performance froms:In the World Cup 2022, [MASK] won people’s praise. . To achieve this, GENIE leverages two components: a bidirectional encoder model and a cross-attention diffusion model. The encoder model encodes the source text sinto a set of hidden vec- torsHs=Encoder (s), which indicates the distributed representation of s. The diffusion model takes Hsand a Gaussian noise as inputs, and iteratively reﬁnes the data by applying a sequence of denoising operations. In contrast to the traditional autoregressive text generation paradigm, which generates one token at a time, the diffusion model in GENIE outputs the sequence of embeddings in parallel at each denoising step, making GENIE a non-autoregressive generation (NAR) model"
        }
      ],
      "metadata": {
        "question_type": "factual",
        "generated_by": "gpt-4o-mini",
        "top_k": 5,
        "retrieved_chunks_array": [
          {
            "chunk_id": "36b636a9f90b4e2c92687fe34ecde216",
            "source": "Text diffusion - 2212.11685v2",
            "chunk_text": ". It is worth noting that GENIE also belongs to the Semi-NAR model. The second group is AR model, the model of encoder- decoder structure including LSTM (Greff et al., 2017), Transformer (Vaswani et al., 2017a), bRNN-CopyNet (Gu et al., 2016a), Trans-CopyNet (Lin et al., 2019),MeanPooling-CopyNet (Lin et al., 2019) without pre- training, and strong baselines MASS (Song et al., 2019), BART (Lewis et al., 2019), T5 (Raffel et al., 2020b), BANG (Qi et al., 2021), and ProphetNet (Qi et al., 2020) with large scale pre-training. For large scale pre-training models mentioned above, we select the base version of the model, which is equivalent to the total number of GENIE parameters. 4.4. Main Results We present the results of GENIE and the baselines on XSUM,CNN/D AILY MAIL,GIGAWORD ,COMMON GENin Table 1, Table 2, and Table 3. Our results demonstrate that the pre-trained GENIE is a powerful NAR model for text generation"
          },
          {
            "chunk_id": "06d0f814bea54edea7f4a6db2cdb635e",
            "source": "Text diffusion - 2212.11685v2",
            "chunk_text": ". We evaluate GENIE on four downstream text generation benchmarks, namely XSUM,CNN/D AILY MAIL,GIGA- WORD , and COMMON GEN. Our experimental results show that GENIE achieves comparable performance with the state-of-the-art autore- gressive models on these benchmarks, and generates more diverse text samples. The code and models of GENIE are available athttps://github.com/microsoft/ ProphetNet/tree/master/GENIE . 1. Introduction Text generation is a crucial task in natural language process- ing, which aims to produce ﬂuent and coherent texts for var- ious applications. Previous text generation methods mainly relied on recurrent neural networks (RNNs) (Pawade et al., 2018; Song et al., 2018; Gu et al., 2016a; Qi et al., 2021), which generate texts sequentially from left to right. How- ever, RNNs suffer from issues such as long-term dependency and exposure bias"
          },
          {
            "chunk_id": "79988d53076a456e9850604a7cf7a162",
            "source": "Text diffusion - 2212.11685v2",
            "chunk_text": ". Common Sense Generation Common sense generation tasks require the model have the ability of generative com- monsense reasoning. Speciﬁcally, given a series of common sense concepts, the model needs to generate coherent state- ments based on these concepts that adhere to real-world scenarios. We select the widely used dataset COMMON - GEN(Lin et al., 2019) to evaluate whether GENIE has good creativity and reasoning ability in natural language generat- ing. In the ﬁne-tuning phase, we set the learning rate to 1e-4 and train 10k steps in total. Finally, we randomly sampled 10 gaussian noises and selected the best sample as the ﬁnal result. Referring to the previous work (Lin et al., 2019), we Text Generation with Diffusion Language Models: A Pre-training Approach with Continuous Paragraph Denoise Table 1. Results of Semi-NAR, NAR and AR on XSUM. Index OVERALL represents the average value of ROUGE-1 ,ROUGE-2 and ROUGE-L . It should be noted that GENIE belongs to Semi-NAR"
          },
          {
            "chunk_id": "e29068c2161f4848aef9465e04553e89",
            "source": "Text diffusion - 2212.11685v2",
            "chunk_text": ". We evaluate GENIE on four popular text generation bench- marks: XSum (Narayan et al., 2018), CNN/DailyMail (Her- mann et al., 2015), Gigaword (Rush et al., 2015), and Com- monGen (Lin et al., 2019). The experimental results demon- strate that GENIE achieves competitive performance witharXiv:2212.11685v2 [cs.CL] 17 Feb 2023 Text Generation with Diffusion Language Models: A Pre-training Approach with Continuous Paragraph Denoise Transformer-based AR methods, and that the proposed pre- training method can effectively improve the performance. We notice that GENIE has achieved signiﬁcant improve- ments in diversity metric. To evaluate the multiple outputs of the generation model, we design an automatic annotation method based on large language model. We also conduct ablation studies to analyze the impact of the diffusion steps and pre-training steps"
          },
          {
            "chunk_id": "6113fe37f3ca4637895495f9a2abb6d2",
            "source": "Text diffusion - 2212.11685v2",
            "chunk_text": ". Model GENIE is the proposed diffusion language model for pre- training, it adopts the sequence-to-sequence framework as illustrated in Figure 1. GENIE could generate a high- quality text sequence ygiven a source text s, such as pro- ducing y:Messi’s performance froms:In the World Cup 2022, [MASK] won people’s praise. . To achieve this, GENIE leverages two components: a bidirectional encoder model and a cross-attention diffusion model. The encoder model encodes the source text sinto a set of hidden vec- torsHs=Encoder (s), which indicates the distributed representation of s. The diffusion model takes Hsand a Gaussian noise as inputs, and iteratively reﬁnes the data by applying a sequence of denoising operations. In contrast to the traditional autoregressive text generation paradigm, which generates one token at a time, the diffusion model in GENIE outputs the sequence of embeddings in parallel at each denoising step, making GENIE a non-autoregressive generation (NAR) model"
          }
        ]
      }
    },
    {
      "question": "What are the four downstream text generation benchmarks on which GENIE was evaluated?",
      "expected_answer": "GENIE was evaluated on the XSUM, CNN/DAILY MAIL, GIGAWORD, and COMMON GEN benchmarks.",
      "actual_answer": "GENIE was evaluated on the following four downstream text generation benchmarks:\n\n1. **XSUM**\n2. **CNN/Daily Mail**\n3. **Gigaword**\n4. **CommonGen**",
      "expected_correct_chunks": ". We evaluate GENIE on four downstream text generation benchmarks, namely XSUM,CNN/D AILY MAIL,GIGA- WORD , and COMMON GEN. Our experimental results show that GENIE achieves comparable performance with the state-of-the-art autore- gressive models on these benchmarks, and generates more diverse text samples. The code and models of GENIE are available athttps://github.com/microsoft/ ProphetNet/tree/master/GENIE . 1. Introduction Text generation is a crucial task in natural language process- ing, which aims to produce ﬂuent and coherent texts for var- ious applications. Previous text generation methods mainly relied on recurrent neural networks (RNNs) (Pawade et al., 2018; Song et al., 2018; Gu et al., 2016a; Qi et al., 2021), which generate texts sequentially from left to right. How- ever, RNNs suffer from issues such as long-term dependency and exposure bias",
      "sources": [
        "Text diffusion - 2212.11685v2",
        "Text diffusion - 2212.11685v2",
        "Text diffusion - 2212.11685v2",
        "Text diffusion - 2212.11685v2",
        "Text diffusion - 2212.11685v2"
      ],
      "chunk_ids": [
        "06d0f814bea54edea7f4a6db2cdb635e",
        "e29068c2161f4848aef9465e04553e89",
        "e3f05a56879e4543822b40bd72fd6589",
        "835e636703654cb98deb56911f1d4d74",
        "2d9376b7b6ef4dc0b0dbb9b8045c5aa9"
      ],
      "f1_score": 0.4551724137931034,
      "precision_at_k": 0.29464285714285715,
      "recall_at_k": 1.0,
      "mean_cosine_score": 0.6675511643577234,
      "llm_as_judge_score": 1.0,
      "latency": 4.55793023109436,
      "retrieved_chunks_text": ". We evaluate GENIE on four downstream text generation benchmarks, namely XSUM,CNN/D AILY MAIL,GIGA- WORD , and COMMON GEN. Our experimental results show that GENIE achieves comparable performance with the state-of-the-art autore- gressive models on these benchmarks, and generates more diverse text samples. The code and models of GENIE are available athttps://github.com/microsoft/ ProphetNet/tree/master/GENIE . 1. Introduction Text generation is a crucial task in natural language process- ing, which aims to produce ﬂuent and coherent texts for var- ious applications. Previous text generation methods mainly relied on recurrent neural networks (RNNs) (Pawade et al., 2018; Song et al., 2018; Gu et al., 2016a; Qi et al., 2021), which generate texts sequentially from left to right. How- ever, RNNs suffer from issues such as long-term dependency and exposure bias\n. We evaluate GENIE on four popular text generation bench- marks: XSum (Narayan et al., 2018), CNN/DailyMail (Her- mann et al., 2015), Gigaword (Rush et al., 2015), and Com- monGen (Lin et al., 2019). The experimental results demon- strate that GENIE achieves competitive performance witharXiv:2212.11685v2 [cs.CL] 17 Feb 2023 Text Generation with Diffusion Language Models: A Pre-training Approach with Continuous Paragraph Denoise Transformer-based AR methods, and that the proposed pre- training method can effectively improve the performance. We notice that GENIE has achieved signiﬁcant improve- ments in diversity metric. To evaluate the multiple outputs of the generation model, we design an automatic annotation method based on large language model. We also conduct ablation studies to analyze the impact of the diffusion steps and pre-training steps\n. In contrast, the diffusion generation method can maintain the quality of generation while offering rich diversity. However, it may not be fair to compare GENIE directly with the single reference to prove that GENIE can achieve diver- sity without compromising quality. Therefore, we design a new evaluation method. We use text-davinci-003 version of InstructGPT (Ouyang et al., 2022), which is based on the large language model (LLM) GPT-3.5, to score our generated texts, that is, to evaluate the quality of the gen- erated summaries. Speciﬁcally, we ﬁrst obtain the sample set (10 summaries generated by BART using diverse beam search and 10 summaries generated by GENIE ), and design a prompt to input into text-davinci-003 to score the generated summaries, while counting the number of high- quality summaries within the 10 summaries generated by BART and GENIE respectively\n. To measure the diversity of GENIE generation, we use SELF-BLEU as the metric. The lower the SELF-BLEU score, the more diverse the generated texts are. For compar- ison, we use BART, a state-of-the-art autoregressive model, which is pre-trained on large scale corpora. For BART, we apply different decoding methods of autoregressive models, such as greedy search, beam search (Xiao et al., 2022), di- verse beam search (diversity strength = 0.8) (Vijayakumar et al., 2016), typical sampling ( = 1:2) (Meister et al., 2022), top-k sampling ( k= 50 ) (Fan et al., 2018), and nu- cleus sampling ( p= 0:92) (Holtzman et al., 2020). These decoding methods can generate multiple texts from the same source sequence. In this experiment, we generate 10 differ- ent target sequences for each source sequence using GENIE and BART. Then we use the 10 summaries generated from XSUM,CNN/D AILY MAIL, and GIGAWORD to calculate theSELF-BLEU scores\n. We also compare the pre-trained GENIE andGENIE trained from scratch (w/o pre-train). As shown in Table 1 and Ta- ble 2, pre-training signiﬁcantly improves the ROUGE-1 , ROUGE-2 ,ROUGE-L scores of GENIE on the three text summarization datasets. Similarly, the results on COM- MON GENin Table 3 indicate that pre-training enhances the performance of GENIE on this task. These results conﬁrm the effectiveness of our pre-training method. 4.5. Generate Diversity Comparison With the emergence of the diffusion based model such as GENIE , the advantages of text generation in diversity will be gradually valued. In this experiment, we will use both quantitative metrics and qualitative examples to show the richness of GENIE in text generation. To measure the diversity of GENIE generation, we use SELF-BLEU as the metric. The lower the SELF-BLEU score, the more diverse the generated texts are",
      "retrieved_chunks": [
        {
          "chunk_id": "06d0f814bea54edea7f4a6db2cdb635e",
          "source": "Text diffusion - 2212.11685v2",
          "chunk_text": ". We evaluate GENIE on four downstream text generation benchmarks, namely XSUM,CNN/D AILY MAIL,GIGA- WORD , and COMMON GEN. Our experimental results show that GENIE achieves comparable performance with the state-of-the-art autore- gressive models on these benchmarks, and generates more diverse text samples. The code and models of GENIE are available athttps://github.com/microsoft/ ProphetNet/tree/master/GENIE . 1. Introduction Text generation is a crucial task in natural language process- ing, which aims to produce ﬂuent and coherent texts for var- ious applications. Previous text generation methods mainly relied on recurrent neural networks (RNNs) (Pawade et al., 2018; Song et al., 2018; Gu et al., 2016a; Qi et al., 2021), which generate texts sequentially from left to right. How- ever, RNNs suffer from issues such as long-term dependency and exposure bias"
        },
        {
          "chunk_id": "e29068c2161f4848aef9465e04553e89",
          "source": "Text diffusion - 2212.11685v2",
          "chunk_text": ". We evaluate GENIE on four popular text generation bench- marks: XSum (Narayan et al., 2018), CNN/DailyMail (Her- mann et al., 2015), Gigaword (Rush et al., 2015), and Com- monGen (Lin et al., 2019). The experimental results demon- strate that GENIE achieves competitive performance witharXiv:2212.11685v2 [cs.CL] 17 Feb 2023 Text Generation with Diffusion Language Models: A Pre-training Approach with Continuous Paragraph Denoise Transformer-based AR methods, and that the proposed pre- training method can effectively improve the performance. We notice that GENIE has achieved signiﬁcant improve- ments in diversity metric. To evaluate the multiple outputs of the generation model, we design an automatic annotation method based on large language model. We also conduct ablation studies to analyze the impact of the diffusion steps and pre-training steps"
        },
        {
          "chunk_id": "e3f05a56879e4543822b40bd72fd6589",
          "source": "Text diffusion - 2212.11685v2",
          "chunk_text": ". In contrast, the diffusion generation method can maintain the quality of generation while offering rich diversity. However, it may not be fair to compare GENIE directly with the single reference to prove that GENIE can achieve diver- sity without compromising quality. Therefore, we design a new evaluation method. We use text-davinci-003 version of InstructGPT (Ouyang et al., 2022), which is based on the large language model (LLM) GPT-3.5, to score our generated texts, that is, to evaluate the quality of the gen- erated summaries. Speciﬁcally, we ﬁrst obtain the sample set (10 summaries generated by BART using diverse beam search and 10 summaries generated by GENIE ), and design a prompt to input into text-davinci-003 to score the generated summaries, while counting the number of high- quality summaries within the 10 summaries generated by BART and GENIE respectively"
        },
        {
          "chunk_id": "835e636703654cb98deb56911f1d4d74",
          "source": "Text diffusion - 2212.11685v2",
          "chunk_text": ". To measure the diversity of GENIE generation, we use SELF-BLEU as the metric. The lower the SELF-BLEU score, the more diverse the generated texts are. For compar- ison, we use BART, a state-of-the-art autoregressive model, which is pre-trained on large scale corpora. For BART, we apply different decoding methods of autoregressive models, such as greedy search, beam search (Xiao et al., 2022), di- verse beam search (diversity strength = 0.8) (Vijayakumar et al., 2016), typical sampling ( = 1:2) (Meister et al., 2022), top-k sampling ( k= 50 ) (Fan et al., 2018), and nu- cleus sampling ( p= 0:92) (Holtzman et al., 2020). These decoding methods can generate multiple texts from the same source sequence. In this experiment, we generate 10 differ- ent target sequences for each source sequence using GENIE and BART. Then we use the 10 summaries generated from XSUM,CNN/D AILY MAIL, and GIGAWORD to calculate theSELF-BLEU scores"
        },
        {
          "chunk_id": "2d9376b7b6ef4dc0b0dbb9b8045c5aa9",
          "source": "Text diffusion - 2212.11685v2",
          "chunk_text": ". We also compare the pre-trained GENIE andGENIE trained from scratch (w/o pre-train). As shown in Table 1 and Ta- ble 2, pre-training signiﬁcantly improves the ROUGE-1 , ROUGE-2 ,ROUGE-L scores of GENIE on the three text summarization datasets. Similarly, the results on COM- MON GENin Table 3 indicate that pre-training enhances the performance of GENIE on this task. These results conﬁrm the effectiveness of our pre-training method. 4.5. Generate Diversity Comparison With the emergence of the diffusion based model such as GENIE , the advantages of text generation in diversity will be gradually valued. In this experiment, we will use both quantitative metrics and qualitative examples to show the richness of GENIE in text generation. To measure the diversity of GENIE generation, we use SELF-BLEU as the metric. The lower the SELF-BLEU score, the more diverse the generated texts are"
        }
      ],
      "metadata": {
        "question_type": "factual",
        "generated_by": "gpt-4o-mini",
        "top_k": 5,
        "retrieved_chunks_array": [
          {
            "chunk_id": "06d0f814bea54edea7f4a6db2cdb635e",
            "source": "Text diffusion - 2212.11685v2",
            "chunk_text": ". We evaluate GENIE on four downstream text generation benchmarks, namely XSUM,CNN/D AILY MAIL,GIGA- WORD , and COMMON GEN. Our experimental results show that GENIE achieves comparable performance with the state-of-the-art autore- gressive models on these benchmarks, and generates more diverse text samples. The code and models of GENIE are available athttps://github.com/microsoft/ ProphetNet/tree/master/GENIE . 1. Introduction Text generation is a crucial task in natural language process- ing, which aims to produce ﬂuent and coherent texts for var- ious applications. Previous text generation methods mainly relied on recurrent neural networks (RNNs) (Pawade et al., 2018; Song et al., 2018; Gu et al., 2016a; Qi et al., 2021), which generate texts sequentially from left to right. How- ever, RNNs suffer from issues such as long-term dependency and exposure bias"
          },
          {
            "chunk_id": "e29068c2161f4848aef9465e04553e89",
            "source": "Text diffusion - 2212.11685v2",
            "chunk_text": ". We evaluate GENIE on four popular text generation bench- marks: XSum (Narayan et al., 2018), CNN/DailyMail (Her- mann et al., 2015), Gigaword (Rush et al., 2015), and Com- monGen (Lin et al., 2019). The experimental results demon- strate that GENIE achieves competitive performance witharXiv:2212.11685v2 [cs.CL] 17 Feb 2023 Text Generation with Diffusion Language Models: A Pre-training Approach with Continuous Paragraph Denoise Transformer-based AR methods, and that the proposed pre- training method can effectively improve the performance. We notice that GENIE has achieved signiﬁcant improve- ments in diversity metric. To evaluate the multiple outputs of the generation model, we design an automatic annotation method based on large language model. We also conduct ablation studies to analyze the impact of the diffusion steps and pre-training steps"
          },
          {
            "chunk_id": "e3f05a56879e4543822b40bd72fd6589",
            "source": "Text diffusion - 2212.11685v2",
            "chunk_text": ". In contrast, the diffusion generation method can maintain the quality of generation while offering rich diversity. However, it may not be fair to compare GENIE directly with the single reference to prove that GENIE can achieve diver- sity without compromising quality. Therefore, we design a new evaluation method. We use text-davinci-003 version of InstructGPT (Ouyang et al., 2022), which is based on the large language model (LLM) GPT-3.5, to score our generated texts, that is, to evaluate the quality of the gen- erated summaries. Speciﬁcally, we ﬁrst obtain the sample set (10 summaries generated by BART using diverse beam search and 10 summaries generated by GENIE ), and design a prompt to input into text-davinci-003 to score the generated summaries, while counting the number of high- quality summaries within the 10 summaries generated by BART and GENIE respectively"
          },
          {
            "chunk_id": "835e636703654cb98deb56911f1d4d74",
            "source": "Text diffusion - 2212.11685v2",
            "chunk_text": ". To measure the diversity of GENIE generation, we use SELF-BLEU as the metric. The lower the SELF-BLEU score, the more diverse the generated texts are. For compar- ison, we use BART, a state-of-the-art autoregressive model, which is pre-trained on large scale corpora. For BART, we apply different decoding methods of autoregressive models, such as greedy search, beam search (Xiao et al., 2022), di- verse beam search (diversity strength = 0.8) (Vijayakumar et al., 2016), typical sampling ( = 1:2) (Meister et al., 2022), top-k sampling ( k= 50 ) (Fan et al., 2018), and nu- cleus sampling ( p= 0:92) (Holtzman et al., 2020). These decoding methods can generate multiple texts from the same source sequence. In this experiment, we generate 10 differ- ent target sequences for each source sequence using GENIE and BART. Then we use the 10 summaries generated from XSUM,CNN/D AILY MAIL, and GIGAWORD to calculate theSELF-BLEU scores"
          },
          {
            "chunk_id": "2d9376b7b6ef4dc0b0dbb9b8045c5aa9",
            "source": "Text diffusion - 2212.11685v2",
            "chunk_text": ". We also compare the pre-trained GENIE andGENIE trained from scratch (w/o pre-train). As shown in Table 1 and Ta- ble 2, pre-training signiﬁcantly improves the ROUGE-1 , ROUGE-2 ,ROUGE-L scores of GENIE on the three text summarization datasets. Similarly, the results on COM- MON GENin Table 3 indicate that pre-training enhances the performance of GENIE on this task. These results conﬁrm the effectiveness of our pre-training method. 4.5. Generate Diversity Comparison With the emergence of the diffusion based model such as GENIE , the advantages of text generation in diversity will be gradually valued. In this experiment, we will use both quantitative metrics and qualitative examples to show the richness of GENIE in text generation. To measure the diversity of GENIE generation, we use SELF-BLEU as the metric. The lower the SELF-BLEU score, the more diverse the generated texts are"
          }
        ]
      }
    },
    {
      "question": "What are some of the technical improvements made in Seedream 3.0 compared to Seedream 2.0?",
      "expected_answer": "Seedream 3.0 includes advancements that address alignment with complicated prompts, fine-grained typography generation, suboptimal visual aesthetics and fidelity, and limited image resolutions. These improvements span the entire pipeline from data construction to model deployment.",
      "actual_answer": "Seedream 3.0 introduces several technical improvements over Seedream 2.0, including:\n\n1. **Defect-Aware Training Paradigm**: Seedream 3.0 employs a specialized defect detector trained on 15,000 manually annotated samples, which helps in addressing limitations due to data filtering in Seedream 2.0. This approach allows for better handling of image data that previously exhibited minor artifacts, such as watermarks and subtitles.\n\n2. **Enhanced Text Rendering**: There is a significant improvement in text rendering performance, especially for generating small-size text characters in both Chinese and English. Seedream 3.0 excels in creating high-aesthetic long-text layouts, making it a pioneering solution for small-text generation and aesthetically pleasing long-text composition.\n\n3. **High-Resolution Output**: Seedream 3.0 provides native high-resolution output, supporting up to 2K resolution, which enhances the visual quality of the generated images.\n\n4. **Acceleration Paradigm**: The model introduces a novel acceleration strategy using consistent noise expectation and importance-aware timestep sampling, resulting in a 4 to 8 times speedup in generation while maintaining image quality.\n\n5. **Comprehensive Capability Enhancement**: Seedream 3.0 demonstrates improvements in key capabilities, such as text-image alignment, compositional structure, aesthetic quality, and text rendering, showing strong user preference.\n\n6. **Post-Training Optimizations**: The model uses diversified aesthetic captions in supervised fine-tuning (SFT) and a vision-language model (VLM)-based reward model with scaling, aligning outputs better with human preferences.\n\nThese improvements collectively lead to a significant upgrade in the overall capabilities of Seedream 3.0 compared to Seedream 2.0, enhancing its utility as a powerful productivity tool.",
      "expected_correct_chunks": "arXiv:2504.11346v3 [cs.CV] 28 Jun 2025 Seedream 3.0 Technical Report ByteDance Seed Abstract We present Seedream 3.0, a high-performance Chinese-English bilingual image generation founda- tion model. We develop several technical improvements to address existing challenges in Seedream 2.0, including alignment with complicated prompts, fine-grained typography generation, suboptimal visual aesthetics and fidelity, and limited image resolutions. Specifically, the advancements of Seedream 3.0 stem from improvements across the entire pipeline, from data construction to model deployment. At the data stratum, we double the dataset using a defect-aware training paradigm and a dual-axis collaborative data-sampling framework. Furthermore, we adopt several effective techniques such as mixed-resolution training, cross-modality RoPE, representation alignment loss, and resolution-aware timestep sampling in the pre-training phase",
      "sources": [
        "Seedream 3.0 - 2504.11346v3",
        "Seedream 3.0 - 2504.11346v3",
        "Seedream 3.0 - 2504.11346v3",
        "Seedream 3.0 - 2504.11346v3",
        "Seedream 3.0 - 2504.11346v3"
      ],
      "chunk_ids": [
        "fbadf46c027d4c23a2555ea3ca552de1",
        "88aaf79098174495b82f576712f0d95b",
        "ce4299a8126b407bb77af547b18d1503",
        "377e3eb8c89642b6b898910bb87c9b87",
        "6e87f9afaac44ca9a2d7974c19961cb3"
      ],
      "f1_score": 0.19325842696629214,
      "precision_at_k": 0.12112676056338029,
      "recall_at_k": 0.4777777777777778,
      "mean_cosine_score": 0.7088073153918678,
      "llm_as_judge_score": 1.0,
      "latency": 8.141034126281738,
      "retrieved_chunks_text": ". Finally, in model acceleration, we encourage stable sampling via consistent noise expectation, effectively reducing the number of function evaluations (NFE) during inference. Compared to Seedream 2.0, Seedream 3.0 shows significant advances in multiple dimensions: •Comprehensive capability enhancement ：Demonstrates strong user preference and significant advancements in key capabilities, including text-image alignment, compositional structure, aesthetic quality and text rendering. •Enhanced text rendering performance : Achieves significantly enhanced text rendering performance, particularly excelling in generating small-size text characters in both Chinese and English, and high- aesthetic long-text layouts. Seedream 3.0 represents a pioneering solution for the challenges of small-text generation and aesthetically pleasing long-text composition, outperforming human-designed templates from platforms like Canva in graphic design output\n. In addition, it provides native high-resolution output (up to 2K), allowing it to generate images with high visual quality. Seedream 3.0 is now accessible on Volcano Engineα. Official Page: https://team.doubao.com/tech/seedream3_0 αModel ID: Doubao-Seedream-3.0-t2i Overall (Elo)AlignmentStructure Aesthetics EvalMuse TextAvailPortraitSeedream 2.0 Imagen 3 Ideogram 3.0 Midjourney v6.1 FLUX1.1 Pro Seedream 3.0 Figure 1 Seedream 3.0 demonstrates outstanding performance across all evaluation aspects. Due to missing data, the Portrait result of Imagen 3 and overall result of Seedream 2.0 are represented by the average values of other models. In addition, Seedream 3.0 ranks first at Artificial Analysis Text to Image Model Leaderboard with an Arena ELO score of 1158 at 17.0K Appearances at the time of publication1. 1https://artificialanalysis.ai/text-to-image/arena?tab=Leaderboard 1 Figure 2 Seedream 3.0 visualization. 2 Contents 1 Introduction . . . . . . . . . . . . . . . . . . . . . . .\n. We fervently hope that Seedream 3.0 can become a practical tool to improve productivity in all aspects of work and daily life. 1https://www.doubao.com/chat/create-image 2https://jimeng.jianying.com/ai-tool/image/generate 4 2 Technical Details 2.1 Data In Seedream 2.0, we employ a stringent data filtering strategy that systematically excluded image data exhibiting minor artifacts, including watermarks, overlaid text, subtitles, and mosaic patterns. This strict filtering protocol significantly limited the amount of data used in the training, especially considering that such affected samples constituted a substantial portion of the original dataset (approximately 35% of the total collection). To address this limitation, Seedream 3.0 introduces an innovative defect-aware training paradigm. This paradigm includes a specialized defect detector trained on 15,000 manually annotated samples selected by an active learning engine\n. During the post-training stage, we utilize diversified aesthetic captions in SFT, and a VLM-based reward model with scaling, thereby achieving outputs that well align with human preferences. Furthermore, See- dream 3.0 pioneers a novel acceleration paradigm. By employing consistent noise expectation and importance-aware timestep sampling, we achieve a 4 to 8 times speedup while maintaining image quality. Seedream 3.0 demonstrates significant improvements over Seedream 2.0: it enhances overall capabilities, in particular for text-rendering in complicated Chinese characters which is important to professional typography generation. In addition, it provides native high-resolution output (up to 2K), allowing it to generate images with high visual quality. Seedream 3.0 is now accessible on Volcano Engineα\n. 18 Figure 20 Image Quality Comparisons. Left: Seedream 3.0, Right: GPT-4o. 4 Conclusion In this paper, we have introduced Seedream 3.0, which employs several innovative strategies to address existing challenges, including limited image resolutions, complex attributes adherence, fine-grained typography genera- tion, and suboptimal visual aesthetics and fidelity. Through system-level upgrades in data construction, model pretraining, post-training, and model acceleration, Seedream 3.0 has achieved comprehensive improvements in multiple aspects compared to our previous version. Seedream 3.0 provides native high-resolution output, comprehensive capability, superior text rendering quality, enhanced visual appeal, and extreme generation speed. With its integration into platforms like Doubao and Jimeng, Seedream 3.0 exhibits strong potential to become a powerful productivity tool across various work and daily life scenarios. 19 References [1] artificialanalysis.ai. artificialanalysis",
      "retrieved_chunks": [
        {
          "chunk_id": "fbadf46c027d4c23a2555ea3ca552de1",
          "source": "Seedream 3.0 - 2504.11346v3",
          "chunk_text": ". Finally, in model acceleration, we encourage stable sampling via consistent noise expectation, effectively reducing the number of function evaluations (NFE) during inference. Compared to Seedream 2.0, Seedream 3.0 shows significant advances in multiple dimensions: •Comprehensive capability enhancement ：Demonstrates strong user preference and significant advancements in key capabilities, including text-image alignment, compositional structure, aesthetic quality and text rendering. •Enhanced text rendering performance : Achieves significantly enhanced text rendering performance, particularly excelling in generating small-size text characters in both Chinese and English, and high- aesthetic long-text layouts. Seedream 3.0 represents a pioneering solution for the challenges of small-text generation and aesthetically pleasing long-text composition, outperforming human-designed templates from platforms like Canva in graphic design output"
        },
        {
          "chunk_id": "88aaf79098174495b82f576712f0d95b",
          "source": "Seedream 3.0 - 2504.11346v3",
          "chunk_text": ". In addition, it provides native high-resolution output (up to 2K), allowing it to generate images with high visual quality. Seedream 3.0 is now accessible on Volcano Engineα. Official Page: https://team.doubao.com/tech/seedream3_0 αModel ID: Doubao-Seedream-3.0-t2i Overall (Elo)AlignmentStructure Aesthetics EvalMuse TextAvailPortraitSeedream 2.0 Imagen 3 Ideogram 3.0 Midjourney v6.1 FLUX1.1 Pro Seedream 3.0 Figure 1 Seedream 3.0 demonstrates outstanding performance across all evaluation aspects. Due to missing data, the Portrait result of Imagen 3 and overall result of Seedream 2.0 are represented by the average values of other models. In addition, Seedream 3.0 ranks first at Artificial Analysis Text to Image Model Leaderboard with an Arena ELO score of 1158 at 17.0K Appearances at the time of publication1. 1https://artificialanalysis.ai/text-to-image/arena?tab=Leaderboard 1 Figure 2 Seedream 3.0 visualization. 2 Contents 1 Introduction . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
          "chunk_id": "ce4299a8126b407bb77af547b18d1503",
          "source": "Seedream 3.0 - 2504.11346v3",
          "chunk_text": ". We fervently hope that Seedream 3.0 can become a practical tool to improve productivity in all aspects of work and daily life. 1https://www.doubao.com/chat/create-image 2https://jimeng.jianying.com/ai-tool/image/generate 4 2 Technical Details 2.1 Data In Seedream 2.0, we employ a stringent data filtering strategy that systematically excluded image data exhibiting minor artifacts, including watermarks, overlaid text, subtitles, and mosaic patterns. This strict filtering protocol significantly limited the amount of data used in the training, especially considering that such affected samples constituted a substantial portion of the original dataset (approximately 35% of the total collection). To address this limitation, Seedream 3.0 introduces an innovative defect-aware training paradigm. This paradigm includes a specialized defect detector trained on 15,000 manually annotated samples selected by an active learning engine"
        },
        {
          "chunk_id": "377e3eb8c89642b6b898910bb87c9b87",
          "source": "Seedream 3.0 - 2504.11346v3",
          "chunk_text": ". During the post-training stage, we utilize diversified aesthetic captions in SFT, and a VLM-based reward model with scaling, thereby achieving outputs that well align with human preferences. Furthermore, See- dream 3.0 pioneers a novel acceleration paradigm. By employing consistent noise expectation and importance-aware timestep sampling, we achieve a 4 to 8 times speedup while maintaining image quality. Seedream 3.0 demonstrates significant improvements over Seedream 2.0: it enhances overall capabilities, in particular for text-rendering in complicated Chinese characters which is important to professional typography generation. In addition, it provides native high-resolution output (up to 2K), allowing it to generate images with high visual quality. Seedream 3.0 is now accessible on Volcano Engineα"
        },
        {
          "chunk_id": "6e87f9afaac44ca9a2d7974c19961cb3",
          "source": "Seedream 3.0 - 2504.11346v3",
          "chunk_text": ". 18 Figure 20 Image Quality Comparisons. Left: Seedream 3.0, Right: GPT-4o. 4 Conclusion In this paper, we have introduced Seedream 3.0, which employs several innovative strategies to address existing challenges, including limited image resolutions, complex attributes adherence, fine-grained typography genera- tion, and suboptimal visual aesthetics and fidelity. Through system-level upgrades in data construction, model pretraining, post-training, and model acceleration, Seedream 3.0 has achieved comprehensive improvements in multiple aspects compared to our previous version. Seedream 3.0 provides native high-resolution output, comprehensive capability, superior text rendering quality, enhanced visual appeal, and extreme generation speed. With its integration into platforms like Doubao and Jimeng, Seedream 3.0 exhibits strong potential to become a powerful productivity tool across various work and daily life scenarios. 19 References [1] artificialanalysis.ai. artificialanalysis"
        }
      ],
      "metadata": {
        "question_type": "factual",
        "generated_by": "gpt-4o-mini",
        "top_k": 5,
        "retrieved_chunks_array": [
          {
            "chunk_id": "fbadf46c027d4c23a2555ea3ca552de1",
            "source": "Seedream 3.0 - 2504.11346v3",
            "chunk_text": ". Finally, in model acceleration, we encourage stable sampling via consistent noise expectation, effectively reducing the number of function evaluations (NFE) during inference. Compared to Seedream 2.0, Seedream 3.0 shows significant advances in multiple dimensions: •Comprehensive capability enhancement ：Demonstrates strong user preference and significant advancements in key capabilities, including text-image alignment, compositional structure, aesthetic quality and text rendering. •Enhanced text rendering performance : Achieves significantly enhanced text rendering performance, particularly excelling in generating small-size text characters in both Chinese and English, and high- aesthetic long-text layouts. Seedream 3.0 represents a pioneering solution for the challenges of small-text generation and aesthetically pleasing long-text composition, outperforming human-designed templates from platforms like Canva in graphic design output"
          },
          {
            "chunk_id": "88aaf79098174495b82f576712f0d95b",
            "source": "Seedream 3.0 - 2504.11346v3",
            "chunk_text": ". In addition, it provides native high-resolution output (up to 2K), allowing it to generate images with high visual quality. Seedream 3.0 is now accessible on Volcano Engineα. Official Page: https://team.doubao.com/tech/seedream3_0 αModel ID: Doubao-Seedream-3.0-t2i Overall (Elo)AlignmentStructure Aesthetics EvalMuse TextAvailPortraitSeedream 2.0 Imagen 3 Ideogram 3.0 Midjourney v6.1 FLUX1.1 Pro Seedream 3.0 Figure 1 Seedream 3.0 demonstrates outstanding performance across all evaluation aspects. Due to missing data, the Portrait result of Imagen 3 and overall result of Seedream 2.0 are represented by the average values of other models. In addition, Seedream 3.0 ranks first at Artificial Analysis Text to Image Model Leaderboard with an Arena ELO score of 1158 at 17.0K Appearances at the time of publication1. 1https://artificialanalysis.ai/text-to-image/arena?tab=Leaderboard 1 Figure 2 Seedream 3.0 visualization. 2 Contents 1 Introduction . . . . . . . . . . . . . . . . . . . . . . ."
          },
          {
            "chunk_id": "ce4299a8126b407bb77af547b18d1503",
            "source": "Seedream 3.0 - 2504.11346v3",
            "chunk_text": ". We fervently hope that Seedream 3.0 can become a practical tool to improve productivity in all aspects of work and daily life. 1https://www.doubao.com/chat/create-image 2https://jimeng.jianying.com/ai-tool/image/generate 4 2 Technical Details 2.1 Data In Seedream 2.0, we employ a stringent data filtering strategy that systematically excluded image data exhibiting minor artifacts, including watermarks, overlaid text, subtitles, and mosaic patterns. This strict filtering protocol significantly limited the amount of data used in the training, especially considering that such affected samples constituted a substantial portion of the original dataset (approximately 35% of the total collection). To address this limitation, Seedream 3.0 introduces an innovative defect-aware training paradigm. This paradigm includes a specialized defect detector trained on 15,000 manually annotated samples selected by an active learning engine"
          },
          {
            "chunk_id": "377e3eb8c89642b6b898910bb87c9b87",
            "source": "Seedream 3.0 - 2504.11346v3",
            "chunk_text": ". During the post-training stage, we utilize diversified aesthetic captions in SFT, and a VLM-based reward model with scaling, thereby achieving outputs that well align with human preferences. Furthermore, See- dream 3.0 pioneers a novel acceleration paradigm. By employing consistent noise expectation and importance-aware timestep sampling, we achieve a 4 to 8 times speedup while maintaining image quality. Seedream 3.0 demonstrates significant improvements over Seedream 2.0: it enhances overall capabilities, in particular for text-rendering in complicated Chinese characters which is important to professional typography generation. In addition, it provides native high-resolution output (up to 2K), allowing it to generate images with high visual quality. Seedream 3.0 is now accessible on Volcano Engineα"
          },
          {
            "chunk_id": "6e87f9afaac44ca9a2d7974c19961cb3",
            "source": "Seedream 3.0 - 2504.11346v3",
            "chunk_text": ". 18 Figure 20 Image Quality Comparisons. Left: Seedream 3.0, Right: GPT-4o. 4 Conclusion In this paper, we have introduced Seedream 3.0, which employs several innovative strategies to address existing challenges, including limited image resolutions, complex attributes adherence, fine-grained typography genera- tion, and suboptimal visual aesthetics and fidelity. Through system-level upgrades in data construction, model pretraining, post-training, and model acceleration, Seedream 3.0 has achieved comprehensive improvements in multiple aspects compared to our previous version. Seedream 3.0 provides native high-resolution output, comprehensive capability, superior text rendering quality, enhanced visual appeal, and extreme generation speed. With its integration into platforms like Doubao and Jimeng, Seedream 3.0 exhibits strong potential to become a powerful productivity tool across various work and daily life scenarios. 19 References [1] artificialanalysis.ai. artificialanalysis"
          }
        ]
      }
    },
    {
      "question": "What advancements does Seedream 3.0 offer compared to Seedream 2.0?",
      "expected_answer": "Seedream 3.0 demonstrates significant improvements over Seedream 2.0 by enhancing overall capabilities, particularly for text-rendering in complicated Chinese characters, which is crucial for professional typography generation. Additionally, it provides native high-resolution output of up to 2K, allowing for the generation of images with high visual quality.",
      "actual_answer": "Seedream 3.0 offers several advancements compared to Seedream 2.0, including:\n\n1. **Defect-Aware Training Paradigm**: Unlike Seedream 2.0, which filtered out image data with minor artifacts, Seedream 3.0 uses a specialized defect detector trained on 15,000 manually annotated samples. This approach allows the model to handle image data with artifacts like watermarks and subtitles more effectively.\n\n2. **Enhanced Text Rendering**: Seedream 3.0 significantly improves text rendering performance, particularly excelling in generating small-size text characters in both Chinese and English, and creating high-aesthetic long-text layouts. It outperforms human-designed templates from platforms like Canva in graphic design output.\n\n3. **High-Resolution Output**: The model provides native high-resolution output, supporting up to 2K resolution, which enhances the visual quality of the generated images.\n\n4. **Acceleration Paradigm**: Seedream 3.0 introduces a novel acceleration strategy using consistent noise expectation and importance-aware timestep sampling. This results in a 4 to 8 times speedup in generation while maintaining image quality.\n\n5. **Comprehensive Capability Enhancement**: The model shows strong user preference and significant improvements in key capabilities, including text-image alignment, compositional structure, aesthetic quality, and text rendering.\n\n6. **Post-Training Optimizations**: Seedream 3.0 uses diversified aesthetic captions in supervised fine-tuning and a vision-language model (VLM)-based reward model with scaling to better align outputs with human preferences.\n\nThese enhancements collectively make Seedream 3.0 a more powerful tool, with significant improvements in performance, speed, and image quality over Seedream 2.0.",
      "expected_correct_chunks": ". During the post-training stage, we utilize diversified aesthetic captions in SFT, and a VLM-based reward model with scaling, thereby achieving outputs that well align with human preferences. Furthermore, See- dream 3.0 pioneers a novel acceleration paradigm. By employing consistent noise expectation and importance-aware timestep sampling, we achieve a 4 to 8 times speedup while maintaining image quality. Seedream 3.0 demonstrates significant improvements over Seedream 2.0: it enhances overall capabilities, in particular for text-rendering in complicated Chinese characters which is important to professional typography generation. In addition, it provides native high-resolution output (up to 2K), allowing it to generate images with high visual quality. Seedream 3.0 is now accessible on Volcano Engineα",
      "sources": [
        "Seedream 3.0 - 2504.11346v3",
        "Seedream 3.0 - 2504.11346v3",
        "Seedream 3.0 - 2504.11346v3",
        "Seedream 3.0 - 2504.11346v3",
        "Seedream 3.0 - 2504.11346v3"
      ],
      "chunk_ids": [
        "fbadf46c027d4c23a2555ea3ca552de1",
        "88aaf79098174495b82f576712f0d95b",
        "ce4299a8126b407bb77af547b18d1503",
        "6e87f9afaac44ca9a2d7974c19961cb3",
        "377e3eb8c89642b6b898910bb87c9b87"
      ],
      "f1_score": 0.41163310961968674,
      "precision_at_k": 0.2591549295774648,
      "recall_at_k": 1.0,
      "mean_cosine_score": 0.722909501277256,
      "llm_as_judge_score": 1.0,
      "latency": 5.510418891906738,
      "retrieved_chunks_text": ". Finally, in model acceleration, we encourage stable sampling via consistent noise expectation, effectively reducing the number of function evaluations (NFE) during inference. Compared to Seedream 2.0, Seedream 3.0 shows significant advances in multiple dimensions: •Comprehensive capability enhancement ：Demonstrates strong user preference and significant advancements in key capabilities, including text-image alignment, compositional structure, aesthetic quality and text rendering. •Enhanced text rendering performance : Achieves significantly enhanced text rendering performance, particularly excelling in generating small-size text characters in both Chinese and English, and high- aesthetic long-text layouts. Seedream 3.0 represents a pioneering solution for the challenges of small-text generation and aesthetically pleasing long-text composition, outperforming human-designed templates from platforms like Canva in graphic design output\n. In addition, it provides native high-resolution output (up to 2K), allowing it to generate images with high visual quality. Seedream 3.0 is now accessible on Volcano Engineα. Official Page: https://team.doubao.com/tech/seedream3_0 αModel ID: Doubao-Seedream-3.0-t2i Overall (Elo)AlignmentStructure Aesthetics EvalMuse TextAvailPortraitSeedream 2.0 Imagen 3 Ideogram 3.0 Midjourney v6.1 FLUX1.1 Pro Seedream 3.0 Figure 1 Seedream 3.0 demonstrates outstanding performance across all evaluation aspects. Due to missing data, the Portrait result of Imagen 3 and overall result of Seedream 2.0 are represented by the average values of other models. In addition, Seedream 3.0 ranks first at Artificial Analysis Text to Image Model Leaderboard with an Arena ELO score of 1158 at 17.0K Appearances at the time of publication1. 1https://artificialanalysis.ai/text-to-image/arena?tab=Leaderboard 1 Figure 2 Seedream 3.0 visualization. 2 Contents 1 Introduction . . . . . . . . . . . . . . . . . . . . . . .\n. We fervently hope that Seedream 3.0 can become a practical tool to improve productivity in all aspects of work and daily life. 1https://www.doubao.com/chat/create-image 2https://jimeng.jianying.com/ai-tool/image/generate 4 2 Technical Details 2.1 Data In Seedream 2.0, we employ a stringent data filtering strategy that systematically excluded image data exhibiting minor artifacts, including watermarks, overlaid text, subtitles, and mosaic patterns. This strict filtering protocol significantly limited the amount of data used in the training, especially considering that such affected samples constituted a substantial portion of the original dataset (approximately 35% of the total collection). To address this limitation, Seedream 3.0 introduces an innovative defect-aware training paradigm. This paradigm includes a specialized defect detector trained on 15,000 manually annotated samples selected by an active learning engine\n. 18 Figure 20 Image Quality Comparisons. Left: Seedream 3.0, Right: GPT-4o. 4 Conclusion In this paper, we have introduced Seedream 3.0, which employs several innovative strategies to address existing challenges, including limited image resolutions, complex attributes adherence, fine-grained typography genera- tion, and suboptimal visual aesthetics and fidelity. Through system-level upgrades in data construction, model pretraining, post-training, and model acceleration, Seedream 3.0 has achieved comprehensive improvements in multiple aspects compared to our previous version. Seedream 3.0 provides native high-resolution output, comprehensive capability, superior text rendering quality, enhanced visual appeal, and extreme generation speed. With its integration into platforms like Doubao and Jimeng, Seedream 3.0 exhibits strong potential to become a powerful productivity tool across various work and daily life scenarios. 19 References [1] artificialanalysis.ai. artificialanalysis\n. During the post-training stage, we utilize diversified aesthetic captions in SFT, and a VLM-based reward model with scaling, thereby achieving outputs that well align with human preferences. Furthermore, See- dream 3.0 pioneers a novel acceleration paradigm. By employing consistent noise expectation and importance-aware timestep sampling, we achieve a 4 to 8 times speedup while maintaining image quality. Seedream 3.0 demonstrates significant improvements over Seedream 2.0: it enhances overall capabilities, in particular for text-rendering in complicated Chinese characters which is important to professional typography generation. In addition, it provides native high-resolution output (up to 2K), allowing it to generate images with high visual quality. Seedream 3.0 is now accessible on Volcano Engineα",
      "retrieved_chunks": [
        {
          "chunk_id": "fbadf46c027d4c23a2555ea3ca552de1",
          "source": "Seedream 3.0 - 2504.11346v3",
          "chunk_text": ". Finally, in model acceleration, we encourage stable sampling via consistent noise expectation, effectively reducing the number of function evaluations (NFE) during inference. Compared to Seedream 2.0, Seedream 3.0 shows significant advances in multiple dimensions: •Comprehensive capability enhancement ：Demonstrates strong user preference and significant advancements in key capabilities, including text-image alignment, compositional structure, aesthetic quality and text rendering. •Enhanced text rendering performance : Achieves significantly enhanced text rendering performance, particularly excelling in generating small-size text characters in both Chinese and English, and high- aesthetic long-text layouts. Seedream 3.0 represents a pioneering solution for the challenges of small-text generation and aesthetically pleasing long-text composition, outperforming human-designed templates from platforms like Canva in graphic design output"
        },
        {
          "chunk_id": "88aaf79098174495b82f576712f0d95b",
          "source": "Seedream 3.0 - 2504.11346v3",
          "chunk_text": ". In addition, it provides native high-resolution output (up to 2K), allowing it to generate images with high visual quality. Seedream 3.0 is now accessible on Volcano Engineα. Official Page: https://team.doubao.com/tech/seedream3_0 αModel ID: Doubao-Seedream-3.0-t2i Overall (Elo)AlignmentStructure Aesthetics EvalMuse TextAvailPortraitSeedream 2.0 Imagen 3 Ideogram 3.0 Midjourney v6.1 FLUX1.1 Pro Seedream 3.0 Figure 1 Seedream 3.0 demonstrates outstanding performance across all evaluation aspects. Due to missing data, the Portrait result of Imagen 3 and overall result of Seedream 2.0 are represented by the average values of other models. In addition, Seedream 3.0 ranks first at Artificial Analysis Text to Image Model Leaderboard with an Arena ELO score of 1158 at 17.0K Appearances at the time of publication1. 1https://artificialanalysis.ai/text-to-image/arena?tab=Leaderboard 1 Figure 2 Seedream 3.0 visualization. 2 Contents 1 Introduction . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
          "chunk_id": "ce4299a8126b407bb77af547b18d1503",
          "source": "Seedream 3.0 - 2504.11346v3",
          "chunk_text": ". We fervently hope that Seedream 3.0 can become a practical tool to improve productivity in all aspects of work and daily life. 1https://www.doubao.com/chat/create-image 2https://jimeng.jianying.com/ai-tool/image/generate 4 2 Technical Details 2.1 Data In Seedream 2.0, we employ a stringent data filtering strategy that systematically excluded image data exhibiting minor artifacts, including watermarks, overlaid text, subtitles, and mosaic patterns. This strict filtering protocol significantly limited the amount of data used in the training, especially considering that such affected samples constituted a substantial portion of the original dataset (approximately 35% of the total collection). To address this limitation, Seedream 3.0 introduces an innovative defect-aware training paradigm. This paradigm includes a specialized defect detector trained on 15,000 manually annotated samples selected by an active learning engine"
        },
        {
          "chunk_id": "6e87f9afaac44ca9a2d7974c19961cb3",
          "source": "Seedream 3.0 - 2504.11346v3",
          "chunk_text": ". 18 Figure 20 Image Quality Comparisons. Left: Seedream 3.0, Right: GPT-4o. 4 Conclusion In this paper, we have introduced Seedream 3.0, which employs several innovative strategies to address existing challenges, including limited image resolutions, complex attributes adherence, fine-grained typography genera- tion, and suboptimal visual aesthetics and fidelity. Through system-level upgrades in data construction, model pretraining, post-training, and model acceleration, Seedream 3.0 has achieved comprehensive improvements in multiple aspects compared to our previous version. Seedream 3.0 provides native high-resolution output, comprehensive capability, superior text rendering quality, enhanced visual appeal, and extreme generation speed. With its integration into platforms like Doubao and Jimeng, Seedream 3.0 exhibits strong potential to become a powerful productivity tool across various work and daily life scenarios. 19 References [1] artificialanalysis.ai. artificialanalysis"
        },
        {
          "chunk_id": "377e3eb8c89642b6b898910bb87c9b87",
          "source": "Seedream 3.0 - 2504.11346v3",
          "chunk_text": ". During the post-training stage, we utilize diversified aesthetic captions in SFT, and a VLM-based reward model with scaling, thereby achieving outputs that well align with human preferences. Furthermore, See- dream 3.0 pioneers a novel acceleration paradigm. By employing consistent noise expectation and importance-aware timestep sampling, we achieve a 4 to 8 times speedup while maintaining image quality. Seedream 3.0 demonstrates significant improvements over Seedream 2.0: it enhances overall capabilities, in particular for text-rendering in complicated Chinese characters which is important to professional typography generation. In addition, it provides native high-resolution output (up to 2K), allowing it to generate images with high visual quality. Seedream 3.0 is now accessible on Volcano Engineα"
        }
      ],
      "metadata": {
        "question_type": "analytical",
        "generated_by": "gpt-4o-mini",
        "top_k": 5,
        "retrieved_chunks_array": [
          {
            "chunk_id": "fbadf46c027d4c23a2555ea3ca552de1",
            "source": "Seedream 3.0 - 2504.11346v3",
            "chunk_text": ". Finally, in model acceleration, we encourage stable sampling via consistent noise expectation, effectively reducing the number of function evaluations (NFE) during inference. Compared to Seedream 2.0, Seedream 3.0 shows significant advances in multiple dimensions: •Comprehensive capability enhancement ：Demonstrates strong user preference and significant advancements in key capabilities, including text-image alignment, compositional structure, aesthetic quality and text rendering. •Enhanced text rendering performance : Achieves significantly enhanced text rendering performance, particularly excelling in generating small-size text characters in both Chinese and English, and high- aesthetic long-text layouts. Seedream 3.0 represents a pioneering solution for the challenges of small-text generation and aesthetically pleasing long-text composition, outperforming human-designed templates from platforms like Canva in graphic design output"
          },
          {
            "chunk_id": "88aaf79098174495b82f576712f0d95b",
            "source": "Seedream 3.0 - 2504.11346v3",
            "chunk_text": ". In addition, it provides native high-resolution output (up to 2K), allowing it to generate images with high visual quality. Seedream 3.0 is now accessible on Volcano Engineα. Official Page: https://team.doubao.com/tech/seedream3_0 αModel ID: Doubao-Seedream-3.0-t2i Overall (Elo)AlignmentStructure Aesthetics EvalMuse TextAvailPortraitSeedream 2.0 Imagen 3 Ideogram 3.0 Midjourney v6.1 FLUX1.1 Pro Seedream 3.0 Figure 1 Seedream 3.0 demonstrates outstanding performance across all evaluation aspects. Due to missing data, the Portrait result of Imagen 3 and overall result of Seedream 2.0 are represented by the average values of other models. In addition, Seedream 3.0 ranks first at Artificial Analysis Text to Image Model Leaderboard with an Arena ELO score of 1158 at 17.0K Appearances at the time of publication1. 1https://artificialanalysis.ai/text-to-image/arena?tab=Leaderboard 1 Figure 2 Seedream 3.0 visualization. 2 Contents 1 Introduction . . . . . . . . . . . . . . . . . . . . . . ."
          },
          {
            "chunk_id": "ce4299a8126b407bb77af547b18d1503",
            "source": "Seedream 3.0 - 2504.11346v3",
            "chunk_text": ". We fervently hope that Seedream 3.0 can become a practical tool to improve productivity in all aspects of work and daily life. 1https://www.doubao.com/chat/create-image 2https://jimeng.jianying.com/ai-tool/image/generate 4 2 Technical Details 2.1 Data In Seedream 2.0, we employ a stringent data filtering strategy that systematically excluded image data exhibiting minor artifacts, including watermarks, overlaid text, subtitles, and mosaic patterns. This strict filtering protocol significantly limited the amount of data used in the training, especially considering that such affected samples constituted a substantial portion of the original dataset (approximately 35% of the total collection). To address this limitation, Seedream 3.0 introduces an innovative defect-aware training paradigm. This paradigm includes a specialized defect detector trained on 15,000 manually annotated samples selected by an active learning engine"
          },
          {
            "chunk_id": "6e87f9afaac44ca9a2d7974c19961cb3",
            "source": "Seedream 3.0 - 2504.11346v3",
            "chunk_text": ". 18 Figure 20 Image Quality Comparisons. Left: Seedream 3.0, Right: GPT-4o. 4 Conclusion In this paper, we have introduced Seedream 3.0, which employs several innovative strategies to address existing challenges, including limited image resolutions, complex attributes adherence, fine-grained typography genera- tion, and suboptimal visual aesthetics and fidelity. Through system-level upgrades in data construction, model pretraining, post-training, and model acceleration, Seedream 3.0 has achieved comprehensive improvements in multiple aspects compared to our previous version. Seedream 3.0 provides native high-resolution output, comprehensive capability, superior text rendering quality, enhanced visual appeal, and extreme generation speed. With its integration into platforms like Doubao and Jimeng, Seedream 3.0 exhibits strong potential to become a powerful productivity tool across various work and daily life scenarios. 19 References [1] artificialanalysis.ai. artificialanalysis"
          },
          {
            "chunk_id": "377e3eb8c89642b6b898910bb87c9b87",
            "source": "Seedream 3.0 - 2504.11346v3",
            "chunk_text": ". During the post-training stage, we utilize diversified aesthetic captions in SFT, and a VLM-based reward model with scaling, thereby achieving outputs that well align with human preferences. Furthermore, See- dream 3.0 pioneers a novel acceleration paradigm. By employing consistent noise expectation and importance-aware timestep sampling, we achieve a 4 to 8 times speedup while maintaining image quality. Seedream 3.0 demonstrates significant improvements over Seedream 2.0: it enhances overall capabilities, in particular for text-rendering in complicated Chinese characters which is important to professional typography generation. In addition, it provides native high-resolution output (up to 2K), allowing it to generate images with high visual quality. Seedream 3.0 is now accessible on Volcano Engineα"
          }
        ]
      }
    },
    {
      "question": "What is the main purpose of AlphaEvolve as described in the text?",
      "expected_answer": "The main purpose of AlphaEvolve is to enhance the capabilities of state-of-the-art LLMs on challenging tasks, specifically in tackling open scientific problems and optimizing critical pieces of computational infrastructure.",
      "actual_answer": "The main purpose of AlphaEvolve is to serve as a coding agent for scientific and algorithmic discovery. It focuses on problems where the candidates for discovery can be automatically evaluated, representing these candidates as algorithms. AlphaEvolve uses a set of large language models (LLMs) to generate, critique, and evolve a pool of such algorithms through an evolutionary approach. This process is grounded in code execution and automatic evaluation, allowing AlphaEvolve to discover candidates that are significantly different in syntax and function from the initial pool. By integrating state-of-the-art LLMs with automated evaluation metrics within an evolutionary framework, AlphaEvolve aims to make new discoveries on longstanding mathematical problems and achieve practical improvements to optimized compute stacks.",
      "expected_correct_chunks": "2025-5-14 AlphaEvolve : A coding agent for scientific and algorithmic discovery Alexander Novikov*, Ngân V ˜u*, Marvin Eisenberger*, Emilien Dupont*, Po-Sen Huang*, Adam Zsolt Wagner*, Sergey Shirobokov*, Borislav Kozlovskii*, Francisco J. R. Ruiz, Abbas Mehrabian, M. Pawan Kumar, Abigail See, Swarat Chaudhuri, George Holland, Alex Davies, Sebastian Nowozin, Pushmeet Kohli and Matej Balog* Google DeepMind1 In this white paper, we present AlphaEvolve , an evolutionary coding agent that substantially enhances capabilities of state-of-the-art LLMs on highly challenging tasks such as tackling open scientific problems or optimizing critical pieces of computational infrastructure. AlphaEvolve orchestrates an autonomous pipeline of LLMs, whose task is to improve an algorithm by making direct changes to the code",
      "sources": [
        "AlphaEvolve",
        "AlphaEvolve",
        "AlphaEvolve",
        "AlphaEvolve",
        "AlphaEvolve"
      ],
      "chunk_ids": [
        "371b081021234e5cad535c9a50f33fb6",
        "5b573e07bf1b47b8a5986b07b4c4016a",
        "30880af1e7b24ad8b8e0559195fb0853",
        "07b38166f8aa41a9ac250d229120f57f",
        "4a30090869d34d5c9e92e4c0fec2e470"
      ],
      "f1_score": 0.17535545023696683,
      "precision_at_k": 0.11526479750778816,
      "recall_at_k": 0.36633663366336633,
      "mean_cosine_score": 0.6651338420203396,
      "llm_as_judge_score": 0.75,
      "latency": 3.347642183303833,
      "retrieved_chunks_text": ". AlphaEvolve focuses on the broad spectrum of scientific and engineering 1See Acknowledgments and Author information section.∗Equal contributions. ©2025 Google DeepMind. All rights reserved AlphaEvolve : A coding agent for scientific and algorithmic discovery discovery problems in which the candidates of discovery can be automatically evaluated. It represents the candidates (for example, new mathematical objects or practical heuristics) as algorithms and uses a set of LLMs to generate, critique, and evolve a pool of such algorithms. The LLM-directed evolution process is grounded using code execution and automatic evalua- tion. This evaluation mechanism allows AlphaEvolve to avoid any incorrect suggestions from the base LLM [43]. The evolutionary process in AlphaEvolve leverages modern LLMs’ ability to respond to feedback, enabling the discovery of candidates that are substantially different from the initial candidatepoolinsyntaxandfunction\n. •Evolutionary approach. AlphaEvolve utilizes an evolutionary approach, where previ- ously generated programs are stored in a database and used to obtain better programs in subsequent iterations. To analyze the importance of evolution, we consider an alternative approach, which repeatedly feeds the same initial program to the language model. We refer to this approach as “No evolution”. •Context in prompts. AlphaEvolve uses powerful language models with large context windows, whose output can be improved significantly by providing problem-specific context in the prompt. To test the importance of context, we consider an alternative approach where no explicit context is added to the prompt. We refer to this approach as “No context in the prompt”\n. Thisdesignfacilitatesintegratingitwithexistingcodebaseswhilerequiring only minimal changes, simply by adding special markers ( # EVOLVE-BLOCK-START and# EVOLVE-BLOCK-END ) as comments into the code. 4 AlphaEvolve : A coding agent for scientific and algorithmic discovery Any user-provided code inside such evolution blocks serves as the initial solution to be improved by AlphaEvolve , and the rest of the code forms a skeletonthat ties the evolved pieces together, so that they can be invoked from evaluate . While this initial implementation must be complete, it can be rudimentary—for instance, consisting of single-line functions that return constants of the appropriate types. Flexibility in choosing the abstraction. AlphaEvolve can be applied to the same problem in very different ways—especially when the evolved programs are not the final output but a means to discover solutions\n. AlphaEvolve can also be seen as a test-time compute agent that, through its evolutionary procedure, significantly enhances the capability of the base LLM (compared to, e.g., repeated sampling). On one hand, this can be seen as a compelling demonstration of how machine feedback is able to sustain test-time compute scaling up to regimes where new scientific discoveries and highly valuable practical optimizations are made. On the other hand, a natural next step will be to consider distilling the AlphaEvolve -augmented performance of the base LLMs into the next generation of the base models. This can have intrinsic value and also, likely, uplift the next version of AlphaEvolve . Beyond distillation, it is also intriguing that AlphaEvolve can make practical discoveries that increase the efficiency of its own infrastructure and of (future versions of) its base LLMs\n. 20 AlphaEvolve : A coding agent for scientific and algorithmic discovery 6. Discussion AlphaEvolve demonstrates the surprising power of combining state-of-the-art LLMs with automated evaluation metrics within an evolutionary framework, which can lead to new discoveries on decades-old mathematical problems as well as practical improvements to highly optimized compute stacks. Interestingly, AlphaEvolve often allows approaching the same problem in different ways: searching for the solution directly, finding a function that constructs it from scratch, or evolving a search algorithm to find it. Applying AlphaEvolve in different ways comes with different biases (for example, finding constructive functions may favor discovering highly symmetric objects [80]) and thus can suit different problems. AlphaEvolve can also be seen as a test-time compute agent that, through its evolutionary procedure, significantly enhances the capability of the base LLM (compared to, e.g., repeated sampling)",
      "retrieved_chunks": [
        {
          "chunk_id": "371b081021234e5cad535c9a50f33fb6",
          "source": "AlphaEvolve",
          "chunk_text": ". AlphaEvolve focuses on the broad spectrum of scientific and engineering 1See Acknowledgments and Author information section.∗Equal contributions. ©2025 Google DeepMind. All rights reserved AlphaEvolve : A coding agent for scientific and algorithmic discovery discovery problems in which the candidates of discovery can be automatically evaluated. It represents the candidates (for example, new mathematical objects or practical heuristics) as algorithms and uses a set of LLMs to generate, critique, and evolve a pool of such algorithms. The LLM-directed evolution process is grounded using code execution and automatic evalua- tion. This evaluation mechanism allows AlphaEvolve to avoid any incorrect suggestions from the base LLM [43]. The evolutionary process in AlphaEvolve leverages modern LLMs’ ability to respond to feedback, enabling the discovery of candidates that are substantially different from the initial candidatepoolinsyntaxandfunction"
        },
        {
          "chunk_id": "5b573e07bf1b47b8a5986b07b4c4016a",
          "source": "AlphaEvolve",
          "chunk_text": ". •Evolutionary approach. AlphaEvolve utilizes an evolutionary approach, where previ- ously generated programs are stored in a database and used to obtain better programs in subsequent iterations. To analyze the importance of evolution, we consider an alternative approach, which repeatedly feeds the same initial program to the language model. We refer to this approach as “No evolution”. •Context in prompts. AlphaEvolve uses powerful language models with large context windows, whose output can be improved significantly by providing problem-specific context in the prompt. To test the importance of context, we consider an alternative approach where no explicit context is added to the prompt. We refer to this approach as “No context in the prompt”"
        },
        {
          "chunk_id": "30880af1e7b24ad8b8e0559195fb0853",
          "source": "AlphaEvolve",
          "chunk_text": ". Thisdesignfacilitatesintegratingitwithexistingcodebaseswhilerequiring only minimal changes, simply by adding special markers ( # EVOLVE-BLOCK-START and# EVOLVE-BLOCK-END ) as comments into the code. 4 AlphaEvolve : A coding agent for scientific and algorithmic discovery Any user-provided code inside such evolution blocks serves as the initial solution to be improved by AlphaEvolve , and the rest of the code forms a skeletonthat ties the evolved pieces together, so that they can be invoked from evaluate . While this initial implementation must be complete, it can be rudimentary—for instance, consisting of single-line functions that return constants of the appropriate types. Flexibility in choosing the abstraction. AlphaEvolve can be applied to the same problem in very different ways—especially when the evolved programs are not the final output but a means to discover solutions"
        },
        {
          "chunk_id": "07b38166f8aa41a9ac250d229120f57f",
          "source": "AlphaEvolve",
          "chunk_text": ". AlphaEvolve can also be seen as a test-time compute agent that, through its evolutionary procedure, significantly enhances the capability of the base LLM (compared to, e.g., repeated sampling). On one hand, this can be seen as a compelling demonstration of how machine feedback is able to sustain test-time compute scaling up to regimes where new scientific discoveries and highly valuable practical optimizations are made. On the other hand, a natural next step will be to consider distilling the AlphaEvolve -augmented performance of the base LLMs into the next generation of the base models. This can have intrinsic value and also, likely, uplift the next version of AlphaEvolve . Beyond distillation, it is also intriguing that AlphaEvolve can make practical discoveries that increase the efficiency of its own infrastructure and of (future versions of) its base LLMs"
        },
        {
          "chunk_id": "4a30090869d34d5c9e92e4c0fec2e470",
          "source": "AlphaEvolve",
          "chunk_text": ". 20 AlphaEvolve : A coding agent for scientific and algorithmic discovery 6. Discussion AlphaEvolve demonstrates the surprising power of combining state-of-the-art LLMs with automated evaluation metrics within an evolutionary framework, which can lead to new discoveries on decades-old mathematical problems as well as practical improvements to highly optimized compute stacks. Interestingly, AlphaEvolve often allows approaching the same problem in different ways: searching for the solution directly, finding a function that constructs it from scratch, or evolving a search algorithm to find it. Applying AlphaEvolve in different ways comes with different biases (for example, finding constructive functions may favor discovering highly symmetric objects [80]) and thus can suit different problems. AlphaEvolve can also be seen as a test-time compute agent that, through its evolutionary procedure, significantly enhances the capability of the base LLM (compared to, e.g., repeated sampling)"
        }
      ],
      "metadata": {
        "question_type": "conceptual",
        "generated_by": "gpt-4o-mini",
        "top_k": 5,
        "retrieved_chunks_array": [
          {
            "chunk_id": "371b081021234e5cad535c9a50f33fb6",
            "source": "AlphaEvolve",
            "chunk_text": ". AlphaEvolve focuses on the broad spectrum of scientific and engineering 1See Acknowledgments and Author information section.∗Equal contributions. ©2025 Google DeepMind. All rights reserved AlphaEvolve : A coding agent for scientific and algorithmic discovery discovery problems in which the candidates of discovery can be automatically evaluated. It represents the candidates (for example, new mathematical objects or practical heuristics) as algorithms and uses a set of LLMs to generate, critique, and evolve a pool of such algorithms. The LLM-directed evolution process is grounded using code execution and automatic evalua- tion. This evaluation mechanism allows AlphaEvolve to avoid any incorrect suggestions from the base LLM [43]. The evolutionary process in AlphaEvolve leverages modern LLMs’ ability to respond to feedback, enabling the discovery of candidates that are substantially different from the initial candidatepoolinsyntaxandfunction"
          },
          {
            "chunk_id": "5b573e07bf1b47b8a5986b07b4c4016a",
            "source": "AlphaEvolve",
            "chunk_text": ". •Evolutionary approach. AlphaEvolve utilizes an evolutionary approach, where previ- ously generated programs are stored in a database and used to obtain better programs in subsequent iterations. To analyze the importance of evolution, we consider an alternative approach, which repeatedly feeds the same initial program to the language model. We refer to this approach as “No evolution”. •Context in prompts. AlphaEvolve uses powerful language models with large context windows, whose output can be improved significantly by providing problem-specific context in the prompt. To test the importance of context, we consider an alternative approach where no explicit context is added to the prompt. We refer to this approach as “No context in the prompt”"
          },
          {
            "chunk_id": "30880af1e7b24ad8b8e0559195fb0853",
            "source": "AlphaEvolve",
            "chunk_text": ". Thisdesignfacilitatesintegratingitwithexistingcodebaseswhilerequiring only minimal changes, simply by adding special markers ( # EVOLVE-BLOCK-START and# EVOLVE-BLOCK-END ) as comments into the code. 4 AlphaEvolve : A coding agent for scientific and algorithmic discovery Any user-provided code inside such evolution blocks serves as the initial solution to be improved by AlphaEvolve , and the rest of the code forms a skeletonthat ties the evolved pieces together, so that they can be invoked from evaluate . While this initial implementation must be complete, it can be rudimentary—for instance, consisting of single-line functions that return constants of the appropriate types. Flexibility in choosing the abstraction. AlphaEvolve can be applied to the same problem in very different ways—especially when the evolved programs are not the final output but a means to discover solutions"
          },
          {
            "chunk_id": "07b38166f8aa41a9ac250d229120f57f",
            "source": "AlphaEvolve",
            "chunk_text": ". AlphaEvolve can also be seen as a test-time compute agent that, through its evolutionary procedure, significantly enhances the capability of the base LLM (compared to, e.g., repeated sampling). On one hand, this can be seen as a compelling demonstration of how machine feedback is able to sustain test-time compute scaling up to regimes where new scientific discoveries and highly valuable practical optimizations are made. On the other hand, a natural next step will be to consider distilling the AlphaEvolve -augmented performance of the base LLMs into the next generation of the base models. This can have intrinsic value and also, likely, uplift the next version of AlphaEvolve . Beyond distillation, it is also intriguing that AlphaEvolve can make practical discoveries that increase the efficiency of its own infrastructure and of (future versions of) its base LLMs"
          },
          {
            "chunk_id": "4a30090869d34d5c9e92e4c0fec2e470",
            "source": "AlphaEvolve",
            "chunk_text": ". 20 AlphaEvolve : A coding agent for scientific and algorithmic discovery 6. Discussion AlphaEvolve demonstrates the surprising power of combining state-of-the-art LLMs with automated evaluation metrics within an evolutionary framework, which can lead to new discoveries on decades-old mathematical problems as well as practical improvements to highly optimized compute stacks. Interestingly, AlphaEvolve often allows approaching the same problem in different ways: searching for the solution directly, finding a function that constructs it from scratch, or evolving a search algorithm to find it. Applying AlphaEvolve in different ways comes with different biases (for example, finding constructive functions may favor discovering highly symmetric objects [80]) and thus can suit different problems. AlphaEvolve can also be seen as a test-time compute agent that, through its evolutionary procedure, significantly enhances the capability of the base LLM (compared to, e.g., repeated sampling)"
          }
        ]
      }
    },
    {
      "question": "What are some specific outcomes that AlphaEvolve achieved when optimizing components at Google?",
      "expected_answer": "AlphaEvolve developed a more efficient scheduling algorithm for data centers, found a functionally equivalent simplification in the circuit design of hardware accelerators, and accelerated the training of the LLM underpinning AlphaEvolve itself.",
      "actual_answer": "AlphaEvolve achieved several specific outcomes when optimizing components at Google:\n\n1. **Data Center Scheduling**: AlphaEvolve developed a more efficient scheduling algorithm for Google's data centers. This new heuristic function outperformed the existing production version, recovering on average 0.7% of Google’s fleet-wide compute resources that would otherwise be stranded.\n\n2. **Circuit Design Simplification**: AlphaEvolve found a functionally equivalent simplification in the circuit design of hardware accelerators, which likely led to improvements in efficiency and performance.\n\n3. **Kernel Optimization**: It provided meaningful optimizations to the FlashAttention kernel, achieving a 32% speedup for the configuration of interest. Additionally, it improved the pre- and postprocessing of kernel inputs and outputs, resulting in a 15% speedup in these areas.\n\nThese outcomes demonstrate AlphaEvolve's capability to discover and implement optimizations that enhance the efficiency of large-scale computational systems at Google.",
      "expected_correct_chunks": ". AlphaEvolve orchestrates an autonomous pipeline of LLMs, whose task is to improve an algorithm by making direct changes to the code. Using an evolutionary approach, continuously receiving feedback from one or more evaluators, AlphaEvolve iteratively improves the algorithm, potentially leading to new scientific and practical discoveries. We demonstrate the broad applicability of this approach by applying it to a number of important com- putational problems. When applied to optimizing critical components of large-scale computational stacks at Google, AlphaEvolve developed a more efficient scheduling algorithm for data centers, found a functionally equivalent simplification in the circuit design of hardware accelerators, and acceler- ated the training of the LLM underpinning AlphaEvolve itself",
      "sources": [
        "AlphaEvolve",
        "AlphaEvolve",
        "AlphaEvolve",
        "AlphaEvolve",
        "AlphaEvolve"
      ],
      "chunk_ids": [
        "a30d35a686f442edafd0aa200ba0ea25",
        "18e2698692aa4c918d2570721e788a20",
        "ba994617647c4bf191e0f1d31e812b6f",
        "bdf64e6b5d864b88a22333afaea34c78",
        "56e87650d71d47a2b7a26b1524f41c5a"
      ],
      "f1_score": 0.3926940639269406,
      "precision_at_k": 0.24431818181818182,
      "recall_at_k": 1.0,
      "mean_cosine_score": 0.6958698417965525,
      "llm_as_judge_score": 1.0,
      "latency": 6.619119167327881,
      "retrieved_chunks_text": ". This highlights the potential for synergistic partnerships between AI-driven discovery engines like AlphaEvolve and human mathematical expertise. 3.3. Optimizing Google’s computing ecosystem Inadditiontothescientificapplicationspresentedinprecedingsections, herewedemonstrate howAlphaEvolve has been used to improve performance of mission-critical infrastructure and deliver real-world impact. 3.3.1. Improving data center scheduling Efficiently scheduling compute jobs onto a cluster of machines is a critical optimization problem, particularly at the scale of Google’s data centers, orchestrated by Borg [ 99]. This task involves assigning jobs to available machines based on job resource requirements and machine capacity. Inefficient assignments can result in stranded resources: when a machine can no longer accept jobs because it has run out of one kind of resource (e.g., memory) but still has other resources free (e.g., CPU)\n. We measure the performance of AlphaEvolve ’s heuristic function on an unseen test dataset of recent workloads and capacity to ensure generalization. Observing that AlphaEvolve ’s heuristic function outperforms the one in production, we rolled out AlphaEvolve ’s heuristic function to the entire fleet. Post- deployment measurements across Google’s fleet confirmed the simulator results, revealing that this heuristic function continuously recovers on average 0.7% of Google’s fleet-wide compute resources, which would otherwise be stranded. AlphaEvolve was chosen over a deep reinforcement learning approach because its code solution not only leads to better performance, but also offers clear advantages in interpretability, debuggability, predictability, and ease of deployment—essential qualities for a mission-critical system\n. AlphaEvolve orchestrates an autonomous pipeline of LLMs, whose task is to improve an algorithm by making direct changes to the code. Using an evolutionary approach, continuously receiving feedback from one or more evaluators, AlphaEvolve iteratively improves the algorithm, potentially leading to new scientific and practical discoveries. We demonstrate the broad applicability of this approach by applying it to a number of important com- putational problems. When applied to optimizing critical components of large-scale computational stacks at Google, AlphaEvolve developed a more efficient scheduling algorithm for data centers, found a functionally equivalent simplification in the circuit design of hardware accelerators, and acceler- ated the training of the LLM underpinning AlphaEvolve itself\n. While the use of an automated evaluation metric offers AlphaEvolve a key advantage, it is also a limitation—in particular, it puts tasks that require manual experimentation out of our scope. Because problems in mathematics, computer science, and system optimization typically permit automated evaluation metrics, our efforts on AlphaEvolve focus on these domains. Specifically, we use AlphaEvolve to make progress on several well-known open problems in algorithm design and constructive mathematics, as well as the optimization of critical layers in the large-scale computation stacks at Google. 2 AlphaEvolve : A coding agent for scientific and algorithmic discovery Within algorithm design, we consider the fundamental problem of discovering fast algorithms for multiplying matrices, a problem to which a more specialized AI approach had been applied previously [ 25]\n. AlphaEvolve was able to provide meaningful optimizations for both levels of abstraction exposed by the IR. Firstly, the FlashAttention kernel for the configuration of interest was sped up by 32%. Secondly, AlphaEvolve found improvements in pre- and postprocessing of kernel inputs and outputs, resulting in a 15% speed up in this part. These results demonstrate the ability of AlphaEvolve to optimize compiler-generated code, offering the potential of incorporating discovered optimizations into existing compilers for specific use cases, or, in the longer term, incorporating AlphaEvolve into the compiler workflow itself. 4. Ablations We carried out ablations on two tasks: finding tensor decompositions for faster matrix multiplication (Section 3.1) and computing lower bounds on kissing numbers (Section 3.2), aiming to understand the efficacy of the following components of AlphaEvolve . •Evolutionary approach",
      "retrieved_chunks": [
        {
          "chunk_id": "a30d35a686f442edafd0aa200ba0ea25",
          "source": "AlphaEvolve",
          "chunk_text": ". This highlights the potential for synergistic partnerships between AI-driven discovery engines like AlphaEvolve and human mathematical expertise. 3.3. Optimizing Google’s computing ecosystem Inadditiontothescientificapplicationspresentedinprecedingsections, herewedemonstrate howAlphaEvolve has been used to improve performance of mission-critical infrastructure and deliver real-world impact. 3.3.1. Improving data center scheduling Efficiently scheduling compute jobs onto a cluster of machines is a critical optimization problem, particularly at the scale of Google’s data centers, orchestrated by Borg [ 99]. This task involves assigning jobs to available machines based on job resource requirements and machine capacity. Inefficient assignments can result in stranded resources: when a machine can no longer accept jobs because it has run out of one kind of resource (e.g., memory) but still has other resources free (e.g., CPU)"
        },
        {
          "chunk_id": "18e2698692aa4c918d2570721e788a20",
          "source": "AlphaEvolve",
          "chunk_text": ". We measure the performance of AlphaEvolve ’s heuristic function on an unseen test dataset of recent workloads and capacity to ensure generalization. Observing that AlphaEvolve ’s heuristic function outperforms the one in production, we rolled out AlphaEvolve ’s heuristic function to the entire fleet. Post- deployment measurements across Google’s fleet confirmed the simulator results, revealing that this heuristic function continuously recovers on average 0.7% of Google’s fleet-wide compute resources, which would otherwise be stranded. AlphaEvolve was chosen over a deep reinforcement learning approach because its code solution not only leads to better performance, but also offers clear advantages in interpretability, debuggability, predictability, and ease of deployment—essential qualities for a mission-critical system"
        },
        {
          "chunk_id": "ba994617647c4bf191e0f1d31e812b6f",
          "source": "AlphaEvolve",
          "chunk_text": ". AlphaEvolve orchestrates an autonomous pipeline of LLMs, whose task is to improve an algorithm by making direct changes to the code. Using an evolutionary approach, continuously receiving feedback from one or more evaluators, AlphaEvolve iteratively improves the algorithm, potentially leading to new scientific and practical discoveries. We demonstrate the broad applicability of this approach by applying it to a number of important com- putational problems. When applied to optimizing critical components of large-scale computational stacks at Google, AlphaEvolve developed a more efficient scheduling algorithm for data centers, found a functionally equivalent simplification in the circuit design of hardware accelerators, and acceler- ated the training of the LLM underpinning AlphaEvolve itself"
        },
        {
          "chunk_id": "bdf64e6b5d864b88a22333afaea34c78",
          "source": "AlphaEvolve",
          "chunk_text": ". While the use of an automated evaluation metric offers AlphaEvolve a key advantage, it is also a limitation—in particular, it puts tasks that require manual experimentation out of our scope. Because problems in mathematics, computer science, and system optimization typically permit automated evaluation metrics, our efforts on AlphaEvolve focus on these domains. Specifically, we use AlphaEvolve to make progress on several well-known open problems in algorithm design and constructive mathematics, as well as the optimization of critical layers in the large-scale computation stacks at Google. 2 AlphaEvolve : A coding agent for scientific and algorithmic discovery Within algorithm design, we consider the fundamental problem of discovering fast algorithms for multiplying matrices, a problem to which a more specialized AI approach had been applied previously [ 25]"
        },
        {
          "chunk_id": "56e87650d71d47a2b7a26b1524f41c5a",
          "source": "AlphaEvolve",
          "chunk_text": ". AlphaEvolve was able to provide meaningful optimizations for both levels of abstraction exposed by the IR. Firstly, the FlashAttention kernel for the configuration of interest was sped up by 32%. Secondly, AlphaEvolve found improvements in pre- and postprocessing of kernel inputs and outputs, resulting in a 15% speed up in this part. These results demonstrate the ability of AlphaEvolve to optimize compiler-generated code, offering the potential of incorporating discovered optimizations into existing compilers for specific use cases, or, in the longer term, incorporating AlphaEvolve into the compiler workflow itself. 4. Ablations We carried out ablations on two tasks: finding tensor decompositions for faster matrix multiplication (Section 3.1) and computing lower bounds on kissing numbers (Section 3.2), aiming to understand the efficacy of the following components of AlphaEvolve . •Evolutionary approach"
        }
      ],
      "metadata": {
        "question_type": "factual",
        "generated_by": "gpt-4o-mini",
        "top_k": 5,
        "retrieved_chunks_array": [
          {
            "chunk_id": "a30d35a686f442edafd0aa200ba0ea25",
            "source": "AlphaEvolve",
            "chunk_text": ". This highlights the potential for synergistic partnerships between AI-driven discovery engines like AlphaEvolve and human mathematical expertise. 3.3. Optimizing Google’s computing ecosystem Inadditiontothescientificapplicationspresentedinprecedingsections, herewedemonstrate howAlphaEvolve has been used to improve performance of mission-critical infrastructure and deliver real-world impact. 3.3.1. Improving data center scheduling Efficiently scheduling compute jobs onto a cluster of machines is a critical optimization problem, particularly at the scale of Google’s data centers, orchestrated by Borg [ 99]. This task involves assigning jobs to available machines based on job resource requirements and machine capacity. Inefficient assignments can result in stranded resources: when a machine can no longer accept jobs because it has run out of one kind of resource (e.g., memory) but still has other resources free (e.g., CPU)"
          },
          {
            "chunk_id": "18e2698692aa4c918d2570721e788a20",
            "source": "AlphaEvolve",
            "chunk_text": ". We measure the performance of AlphaEvolve ’s heuristic function on an unseen test dataset of recent workloads and capacity to ensure generalization. Observing that AlphaEvolve ’s heuristic function outperforms the one in production, we rolled out AlphaEvolve ’s heuristic function to the entire fleet. Post- deployment measurements across Google’s fleet confirmed the simulator results, revealing that this heuristic function continuously recovers on average 0.7% of Google’s fleet-wide compute resources, which would otherwise be stranded. AlphaEvolve was chosen over a deep reinforcement learning approach because its code solution not only leads to better performance, but also offers clear advantages in interpretability, debuggability, predictability, and ease of deployment—essential qualities for a mission-critical system"
          },
          {
            "chunk_id": "ba994617647c4bf191e0f1d31e812b6f",
            "source": "AlphaEvolve",
            "chunk_text": ". AlphaEvolve orchestrates an autonomous pipeline of LLMs, whose task is to improve an algorithm by making direct changes to the code. Using an evolutionary approach, continuously receiving feedback from one or more evaluators, AlphaEvolve iteratively improves the algorithm, potentially leading to new scientific and practical discoveries. We demonstrate the broad applicability of this approach by applying it to a number of important com- putational problems. When applied to optimizing critical components of large-scale computational stacks at Google, AlphaEvolve developed a more efficient scheduling algorithm for data centers, found a functionally equivalent simplification in the circuit design of hardware accelerators, and acceler- ated the training of the LLM underpinning AlphaEvolve itself"
          },
          {
            "chunk_id": "bdf64e6b5d864b88a22333afaea34c78",
            "source": "AlphaEvolve",
            "chunk_text": ". While the use of an automated evaluation metric offers AlphaEvolve a key advantage, it is also a limitation—in particular, it puts tasks that require manual experimentation out of our scope. Because problems in mathematics, computer science, and system optimization typically permit automated evaluation metrics, our efforts on AlphaEvolve focus on these domains. Specifically, we use AlphaEvolve to make progress on several well-known open problems in algorithm design and constructive mathematics, as well as the optimization of critical layers in the large-scale computation stacks at Google. 2 AlphaEvolve : A coding agent for scientific and algorithmic discovery Within algorithm design, we consider the fundamental problem of discovering fast algorithms for multiplying matrices, a problem to which a more specialized AI approach had been applied previously [ 25]"
          },
          {
            "chunk_id": "56e87650d71d47a2b7a26b1524f41c5a",
            "source": "AlphaEvolve",
            "chunk_text": ". AlphaEvolve was able to provide meaningful optimizations for both levels of abstraction exposed by the IR. Firstly, the FlashAttention kernel for the configuration of interest was sped up by 32%. Secondly, AlphaEvolve found improvements in pre- and postprocessing of kernel inputs and outputs, resulting in a 15% speed up in this part. These results demonstrate the ability of AlphaEvolve to optimize compiler-generated code, offering the potential of incorporating discovered optimizations into existing compilers for specific use cases, or, in the longer term, incorporating AlphaEvolve into the compiler workflow itself. 4. Ablations We carried out ablations on two tasks: finding tensor decompositions for faster matrix multiplication (Section 3.1) and computing lower bounds on kissing numbers (Section 3.2), aiming to understand the efficacy of the following components of AlphaEvolve . •Evolutionary approach"
          }
        ]
      }
    }
  ]
}